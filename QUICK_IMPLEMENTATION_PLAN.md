# üéØ –ü–õ–ê–ù –†–ï–ê–õ–ò–ó–ê–¶–ò–ò: DogFACS Action Units ‚Üí Emotions

**–ü—Ä–æ–µ–∫—Ç:** Dog FACS Dataset  
**–ó–∞–¥–∞—á–∞:** –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–π —Å–æ–±–∞–∫ (6 –∫–ª–∞—Å—Å) –Ω–∞ –æ—Å–Ω–æ–≤–µ DogFACS Action Units  
**–ù–∞—É—á–Ω–∞—è –±–∞–∑–∞:** Mota-Rojas et al. 2021  
**–î–∞—Ç–∞:** 24 —è–Ω–≤–∞—Ä—è 2026

---

## –§–ê–ó–ê 1: Head Pose Estimation

**–§–∞–π–ª:** `packages/models/head_pose.py` (–ù–û–í–´–ô)

```python
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
# 1. @dataclass HeadPose(yaw, pitch, roll, is_frontal, confidence)
# 2. def estimate_head_pose(keypoints: list[Keypoint]) -> HeadPose
#    - –í—ã—á–∏—Å–ª–∏—Ç—å yaw, pitch, roll –∏–∑ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã—Ö —Ç–æ—á–µ–∫
#    - –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å is_frontal = True –µ—Å–ª–∏ |yaw|<30 AND |pitch|<30 AND |roll|<30
#    - confidence = mean(visibility –≤—Å–µ—Ö keypoints)
# 3. def validate_head_pose(pose: HeadPose) -> bool
```

**–ê–ª–≥–æ—Ä–∏—Ç–º:**
- YAW: atan2(nose.x - eye_center.x, eye_width)
- PITCH: atan2(nose.y - ear_center.y, ear_span)
- ROLL: atan2(right_ear.y - left_ear.y, ear_width)

---

## –§–ê–ó–ê 2: Action Units Computation

**–§–∞–π–ª:** `packages/models/dogfacs.py` (–ù–û–í–´–ô)

```python
# –°—Ç—Ä—É–∫—Ç—É—Ä–∞:
# 1. @dataclass ActionUnits(au101, au145, au25, au26, au301, au401, ead102, ead103, ead104, ad137)
# 2. def compute_action_units(keypoints: list[Keypoint]) -> ActionUnits

# 10 AU (–∫–∞–∂–¥—ã–π ‚àà [0, 1]):
# - AU101: Inner Brow Raiser = (forehead.y - brow.y) / baseline
# - AU145: Blink = 1.0 - eye_opening_ratio
# - AU25: Lips Part = mouth_opening / baseline
# - AU26: Jaw Drop = max(0, (jaw_dist / baseline - 1.2))
# - AU301: Nose Wrinkler = 1.0 - (nose_width / nose_depth)
# - AU401: Upper Lip Raiser = upper_lift / baseline
# - EAD102: Ears Forward = avg(ear_forward_vector)
# - EAD103: Ears Flattener = 1.0 - (ear_height / baseline)
# - EAD104: Ears Rotator = ear_angle_diff / pi
# - AD137: Nose Lick = 1.0 - (nose_to_mouth_dist / mouth_width)

# –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –∫ [0, 1]
```

---

## –§–ê–ó–ê 3: Temporal Aggregation

**–§–∞–π–ª:** `packages/pipeline/temporal_processor.py` (–ù–û–í–´–ô)

```python
# –°—Ç—Ä—É–∫—Ç—É—Ä—ã:
# 1. class TemporalAUBuffer(window_size=30)
#    - au_history: deque(maxlen=30)
#    - add_frame(aus, confidence)
#    - get_aggregated_au() ‚Üí ActionUnits (mean –∑–∞ 30 –∫–∞–¥—Ä–æ–≤)
#    - get_au_variance() ‚Üí dict (variance –∫–∞–∂–¥–æ–≥–æ AU)
#    - is_stable(threshold=0.15) ‚Üí bool

# 2. class TemporalProcessor(window_size=30, head_pose_threshold=30)
#    - process_frame(keypoints, head_pose) ‚Üí Optional[dict]
#      * –§–ò–õ–¨–¢–†: head_pose.is_frontal == False ‚Üí skip
#      * –§–ò–õ–¨–¢–†: mean_visibility < 0.5 ‚Üí skip
#      * –§–ò–õ–¨–¢–†: critical keypoints visibility < 0.3 ‚Üí skip
#      * –í—ã—á–∏—Å–ª–∏—Ç—å AU, –¥–æ–±–∞–≤–∏—Ç—å –≤ –±—É—Ñ–µ—Ä
#      * –í–µ—Ä–Ω—É—Ç—å aggregated AU –µ—Å–ª–∏ stable
#    - process_video_sequence(keypoints_list, pose_list) ‚Üí ActionUnits
```

---

## –§–ê–ó–ê 4: Emotion Classification

**–§–∞–π–ª:** `packages/models/emotion.py` (–ü–ï–†–ï–ü–ò–°–ê–¢–¨)

```python
# –ò–∑–º–µ–Ω–µ–Ω–∏—è:
# 1. –ó–∞–º–µ–Ω–∏—Ç—å EMOTION_CLASSES —Å 4 –Ω–∞ 6 –∫–ª–∞—Å—Å:
#    ['sad', 'angry', 'relaxed', 'happy', 'fearful', 'neutral']
#
# 2. @dataclass EmotionPrediction(emotion, emotion_id, confidence, probabilities, au_scores)
#
# 3. def classify_emotion_from_au(aus: ActionUnits) -> EmotionPrediction
#    Scoring (–∫–∞–∂–¥—ã–π ‚àà [0, 1]):
#
#    happy_score = (
#        au['AU25'] * 0.35 +           # –†–æ—Ç –æ—Ç–∫—Ä—ã—Ç
#        au['EAD102'] * 0.25 +          # –£—à–∏ –≤–ø–µ—Ä–µ–¥
#        au['AU101'] * 0.15 +           # –ë—Ä–æ–≤–∏ –ø–æ–¥–Ω—è—Ç—ã
#        (1 - au['EAD103']) * 0.15 +   # –£—à–∏ –Ω–µ –ø–ª–æ—Å–∫–∏–µ
#        (1 - au['AD137']) * 0.10      # –ù–µ—Ç —Å—Ç—Ä–µ—Å—Å–∞
#    )
#
#    sad_score = (
#        au['EAD103'] * 0.40 +          # –£—à–∏ –ø–ª–æ—Å–∫–∏–µ
#        au['AU145'] * 0.15 +           # –ú–æ—Ä–≥–∞–Ω–∏–µ
#        (1 - au['AU101']) * 0.15 +    # –ë—Ä–æ–≤–∏ –Ω–µ –ø–æ–¥–Ω—è—Ç—ã
#        (1 - au['AU25']) * 0.15 +     # –†–æ—Ç –∑–∞–∫—Ä—ã—Ç
#        au['AD137'] * 0.15             # –õ–∏–∑–∞–Ω–∏–µ
#    )
#
#    angry_score = (
#        ((au['AU25'] + au['AU26']) / 2) * 0.30 +  # –†–æ—Ç –æ—Ç–∫—Ä—ã—Ç+—á–µ–ª—é—Å—Ç—å
#        au['AU401'] * 0.25 +            # –û—Å–∫–∞–ª –∑—É–±–æ–≤
#        au['AU301'] * 0.15 +            # –ù–æ—Å —Å–º–æ—Ä—â–µ–Ω
#        ((au['EAD103'] + au['EAD104']) / 2) * 0.15 +  # –£—à–∏ –≤ –ø–æ–∑–∏—Ü–∏–∏
#        au['AU145'] * 0.15              # –¢–µ–Ω—Å–µ –º–æ—Ä–≥–∞–Ω–∏–µ
#    )
#
#    fearful_score = (
#        au['EAD103'] * 0.30 +           # –£—à–∏ –ø–ª–æ—Å–∫–∏–µ
#        au['AD137'] * 0.25 +            # –û–±–ª–∏–∑—ã–≤–∞–Ω–∏–µ (–ì–õ–ê–í–ù–´–ô –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä —Å—Ç—Ä–µ—Å—Å–∞)
#        au['AU145'] * 0.20 +            # –ß–∞—Å—Ç–æ–µ –º–æ—Ä–≥–∞–Ω–∏–µ
#        au['AU101'] * 0.12 +            # –ë—Ä–æ–≤–∏ –ø–æ–¥–Ω—è—Ç—ã
#        (1 - au['AU25']) * 0.13         # –†–æ—Ç –∑–∞–∫—Ä—ã—Ç
#    )
#
#    relaxed_score = (
#        (1 - sum(au.values())/len(au)) * 0.50 +  # –ú–∏–Ω–∏–º—É–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
#        (1 - au['EAD103']) * 0.15 +    # –£—à–∏ –Ω–µ –ø–ª–æ—Å–∫–∏–µ
#        (1 - au['EAD102']) * 0.15 +    # –£—à–∏ –Ω–µ –≤–ø–µ—Ä–µ–¥
#        (1 - au['AD137']) * 0.10 +     # –ù–µ—Ç —Å—Ç—Ä–µ—Å—Å–∞
#        (1 - au['AU301']) * 0.10       # –ù–æ—Å –Ω–µ –Ω–∞–ø—Ä—É–∂–µ–Ω
#    )
#
#    neutral_score = (
#        (1 - sum(au.values())/len(au)) * 0.70 +
#        (1 - (au['AU25'] + au['AU26']) / 2) * 0.15 +
#        (1 - (au['EAD103'] + au['EAD102']) / 2) * 0.15
#    )
#
# 4. –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å scores ‚Üí probabilities (softmax-like)
# 5. –í—ã–±—Ä–∞—Ç—å best_emotion = max(scores)
# 6. –í–µ—Ä–Ω—É—Ç—å EmotionPrediction —Å probabilities –¥–ª—è –≤—Å–µ—Ö 6
```

---

## –§–ê–ó–ê 5: Pipeline Integration

**–§–∞–π–ª:** `packages/pipeline/inference.py` (–ú–û–î–ò–§–ò–ö–ê–¶–ò–Ø)

```python
# –î–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã:
from packages.models.head_pose import estimate_head_pose
from packages.models.dogfacs import compute_action_units
from packages.pipeline.temporal_processor import TemporalProcessor
from packages.models.emotion import classify_emotion_from_au

# –í –∫–ª–∞—Å—Å InferencePipeline –¥–æ–±–∞–≤–∏—Ç—å:
self.temporal_processor = TemporalProcessor(window_size=30, head_pose_threshold=30)

# –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥:
def process_video_sequence(self, video_frames: list, stride: int = 1):
    """
    –û–±—Ä–∞–±–æ—Ç–∏—Ç—å –≤–∏–¥–µ–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
    
    –ü—Ä–æ—Ü–µ—Å—Å:
    1. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–∞–¥—Ä–∞ (—Å stride):
       - BBox detection
       - Crop dog region
       - Keypoints detection
       - Head pose estimation
       - –î–æ–±–∞–≤–∏—Ç—å –≤ temporal_processor
    2. –í–µ—Ä–Ω—É—Ç—å final emotion prediction
    """
```

---

## –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø: Step-by-Step

### 1. Head Pose (`packages/models/head_pose.py`)
- [ ] –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `HeadPose` dataclass
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `estimate_head_pose(keypoints)` - –≤—ã—á–∏—Å–ª–∏—Ç—å yaw, pitch, roll
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `validate_head_pose(pose)`

### 2. Action Units (`packages/models/dogfacs.py`)
- [ ] –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `ActionUnits` dataclass (10 AU)
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `compute_action_units(keypoints)` - –≤—ã—á–∏—Å–ª–∏—Ç—å –≤—Å–µ 10 AU

### 3. Temporal Processor (`packages/pipeline/temporal_processor.py`)
- [ ] –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `TemporalAUBuffer` - –∏—Å—Ç–æ—Ä–∏—è + –∞–≥—Ä–µ–≥–∞—Ü–∏—è
- [ ] –ù–∞–ø–∏—Å–∞—Ç—å `TemporalProcessor` - —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–∏–µ + –æ–±—Ä–∞–±–æ—Ç–∫–∞

### 4. Emotion Classification (`packages/models/emotion.py`)
- [ ] –ó–∞–º–µ–Ω–∏—Ç—å EMOTION_CLASSES –Ω–∞ 6 –∫–ª–∞—Å—Å
- [ ] –ü–µ—Ä–µ–ø–∏—Å–∞—Ç—å `classify_emotion_from_au()` —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏
- [ ] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é —ç–º–æ—Ü–∏—é

### 5. Integration (`packages/pipeline/inference.py`)
- [ ] –î–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã
- [ ] –î–æ–±–∞–≤–∏—Ç—å `self.temporal_processor`
- [ ] –î–æ–±–∞–≤–∏—Ç—å –º–µ—Ç–æ–¥ `process_video_sequence()`

---

## –ö–õ–Æ–ß–ï–í–´–ï –î–ï–¢–ê–õ–ò

**Head Pose Filtering:**
- –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –∫–∞–¥—Ä—ã –µ—Å–ª–∏ |yaw| > 30¬∞ –∏–ª–∏ |pitch| > 30¬∞ –∏–ª–∏ |roll| > 30¬∞
- –¢–∞–∫–∂–µ –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ mean_visibility < 0.5
- –ü—Ä–æ–ø—É—Å–∫–∞—Ç—å –µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ keypoints –Ω–µ –≤–∏–¥–Ω—ã (visibility < 0.3)

**Temporal Aggregation:**
- –ë—É—Ñ–µ—Ä 30 –∫–∞–¥—Ä–æ–≤ = 1 —Å–µ–∫—É–Ω–¥–∞ @ 30 FPS
- –£—Å—Ä–µ–¥–Ω—è—Ç—å AU –∫–∞–∂–¥—ã–π –∫–∞–¥—Ä
- –ü—Ä–æ–≤–µ—Ä—è—Ç—å stability: variance < 0.15
- –¢–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ AU –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å

**Emotion Scoring:**
- –ö–∞–∂–¥–∞—è —ç–º–æ—Ü–∏—è = weighted sum of AU
- –ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫ [0, 1]
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å softmax –¥–ª—è probabilities

**6 –ö–ª–∞—Å—Å–æ–≤ (–Ω–æ–≤–æ–µ):**
- happy: AU25 + EAD102
- sad: EAD103 + –Ω–∏–∑–∫–∞—è –∞–∫—Ç–∏–≤–∞—Ü–∏—è
- angry: AU25+AU26 + AU401
- fearful: EAD103 + AD137 (–æ–±–ª–∏–∑—ã–≤–∞–Ω–∏–µ - –≥–ª–∞–≤–Ω—ã–π —Å—Ç—Ä–µ—Å—Å –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä)
- relaxed: –º–∏–Ω–∏–º—É–º –∞–∫—Ç–∏–≤–∞—Ü–∏–∏
- neutral: baseline

---

## NOTES

1. **20 keypoints** —É–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –∫–æ–¥–µ (DogFLW subset)
2. **–ù–∞—É—á–Ω–∞—è –±–∞–∑–∞:** Mota-Rojas et al. 2021 - –¢–∞–±–ª–∏—Ü–∞ 2-3
3. **Timeline:** ~11 —á–∞—Å–æ–≤ —Ä–∞–±–æ—Ç—ã
4. **Testing:** unit tests –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –º–æ–¥—É–ª—è
5. **Compatibility:** –≤—Å–µ –Ω–æ–≤–æ–µ –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π pipeline

---

–í—Å–µ –≥–æ—Ç–æ–≤–æ! Claude –º–æ–∂–µ—Ç —Å—Ä–∞–∑—É –Ω–∞—á–∏–Ω–∞—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é! üöÄ
