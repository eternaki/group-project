# Story 12.2: Quality Assessment

**Status:** To Do
**Assignee:** U1 (Danylo Lohachov)
**Priority:** High
**Sprint:** 12 - Statistics & Reporting

---

## Description

As a QA engineer, I want to assess annotation quality.

---

## Acceptance Criteria

- [ ] Auto vs manual agreement calculated
- [ ] Agreement > 85% verified
- [ ] Per-category accuracy reported
- [ ] Quality report generated

---

## Tasks

- [ ] Compare auto vs manual annotations
- [ ] Calculate agreement metrics (IoU, accuracy)
- [ ] Analyze per-emotion agreement
- [ ] Document quality findings

---

## Metrics

| Metric | Target | Actual |
|--------|--------|--------|
| BBox IoU agreement | > 85% | TBD |
| Breed agreement | > 80% | TBD |
| Emotion agreement | > 75% | TBD |
| Overall agreement | > 85% | TBD |

---

## Output

- `docs/reports/quality-assessment.md`
- Agreement metrics table
