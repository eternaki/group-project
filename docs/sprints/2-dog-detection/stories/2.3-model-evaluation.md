# Story 2.3: Model Evaluation

**Status:** To Do
**Assignee:** U3 (Danylo Zherzdiev)
**Priority:** High
**Sprint:** 2 - Dog Detection

---

## Description

As an ML engineer, I want to evaluate the detection model so that I can verify it meets the >85% mAP target.

---

## Acceptance Criteria

- [ ] mAP calculated on test set
- [ ] mAP > 85% achieved
- [ ] Inference speed measured
- [ ] Results documented with visualizations
- [ ] Failure cases analyzed

---

## Tasks

- [ ] Run evaluation on test set
- [ ] Calculate mAP@0.5 and mAP@0.5:0.95
- [ ] Generate confusion matrix
- [ ] Measure inference time (CPU and GPU)
- [ ] Visualize sample predictions
- [ ] Identify and document failure cases
- [ ] Write evaluation report

---

## Evaluation Script

```python
from ultralytics import YOLO

model = YOLO('runs/bbox/yolov8m_dogs/weights/best.pt')

# Evaluate on test set
metrics = model.val(
    data='data/bbox_training/dataset.yaml',
    split='test',
    save_json=True
)

print(f"mAP@0.5: {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
```

---

## Metrics to Report

| Metric | Target | Actual |
|--------|--------|--------|
| mAP@0.5 | > 85% | TBD |
| mAP@0.5:0.95 | > 70% | TBD |
| Precision | > 85% | TBD |
| Recall | > 80% | TBD |
| Inference (GPU) | < 50ms | TBD |
| Inference (CPU) | < 500ms | TBD |

---

## Visualizations

- [ ] Precision-Recall curve
- [ ] Confusion matrix
- [ ] Sample predictions (good and bad)
- [ ] Loss curves during training

---

## Output

- `docs/reports/bbox-evaluation.md`
- `notebooks/bbox_evaluation.ipynb`
