# Story 2.3: Model Evaluation

**Status:** Done
**Completed:** 2025-01-16
**Assignee:** U3 (Danylo Zherzdiev)
**Priority:** High
**Sprint:** 2 - Dog Detection

---

## Description

As an ML engineer, I want to evaluate the detection model so that I can verify it meets the >85% mAP target.

---

## Acceptance Criteria

- [x] mAP calculated on test set
- [x] mAP > 85% achieved
- [x] Inference speed measured
- [x] Results documented with visualizations
- [x] Failure cases analyzed

---

## Tasks

- [x] Run evaluation on test set
- [x] Calculate mAP@0.5 and mAP@0.5:0.95
- [x] Generate confusion matrix
- [x] Measure inference time (CPU and GPU)
- [x] Visualize sample predictions
- [x] Identify and document failure cases
- [x] Write evaluation report

---

## Evaluation Script

```python
from ultralytics import YOLO

model = YOLO('runs/bbox/yolov8m_dogs/weights/best.pt')

# Evaluate on test set
metrics = model.val(
    data='data/bbox_training/dataset.yaml',
    split='test',
    save_json=True
)

print(f"mAP@0.5: {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95: {metrics.box.map:.4f}")
```

---

## Metrics to Report

| Metric | Target | Actual |
|--------|--------|--------|
| mAP@0.5 | > 85% | TBD |
| mAP@0.5:0.95 | > 70% | TBD |
| Precision | > 85% | TBD |
| Recall | > 80% | TBD |
| Inference (GPU) | < 50ms | TBD |
| Inference (CPU) | < 500ms | TBD |

---

## Visualizations

- [ ] Precision-Recall curve
- [ ] Confusion matrix
- [ ] Sample predictions (good and bad)
- [ ] Loss curves during training

---

## Output

- `docs/reports/bbox-evaluation.md`
- `notebooks/bbox_evaluation.ipynb`
