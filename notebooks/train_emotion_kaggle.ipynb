{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dog Keypoints Training + Rule-Based Emotion Classification\n\n**Podejście zgodne z QUICK_IMPLEMENTATION_PLAN.md:**\n\n| Komponent | Źródło | Opis |\n|-----------|--------|------|\n| Keypoints Model | Trenowany na DogFLW | CNN → Heatmaps → 20 keypoints |\n| Action Units | Geometria | 12 AU obliczanych z keypoints |\n| Emotion | Rule-based | Wagi naukowe (DogFACS) |\n\n**Wymagany dataset Kaggle:**\n- `lovodkin/dogflw` - annotacje keypoints (3855 obrazów)\n\n**Po treningu:**\n- `keypoints_dogflw.pt` - model keypoints do pobrania\n\n**Klasyfikacja emocji NIE wymaga treningu** - używa reguł z DogFACS."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n",
    "print(f'PyTorch: {torch.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === ŚCIEŻKI ===\nDOGFLW_PATH = '/kaggle/input/dogflw/DogFLW'\n\n# === KEYPOINTS ===\nNUM_KEYPOINTS_DOGFLW = 46\nNUM_KEYPOINTS_PROJECT = 20\nHEATMAP_SIZE = 64\nIMAGE_SIZE = 256\n\n# === ACTION UNITS ===\nNUM_ACTION_UNITS = 12\nKEYPOINTS_FEATURES = NUM_KEYPOINTS_PROJECT * 3  # 60\n\n# === EMOTIONS (Rule-based - bez treningu) ===\nEMOTION_CLASSES = ['happy', 'sad', 'angry', 'fearful', 'relaxed', 'neutral']\nNUM_EMOTIONS = 6\nNEUTRAL_THRESHOLD = 0.35\n\n# === TRAINING ===\nBATCH_SIZE_KP = 16\nEPOCHS_KP = 50\nLR_KP = 0.001\n\nprint('Config loaded!')\nprint(f'  Keypoints: {NUM_KEYPOINTS_DOGFLW} → {NUM_KEYPOINTS_PROJECT}')\nprint(f'  Action Units: {NUM_ACTION_UNITS}')\nprint(f'  Emotions: {NUM_EMOTIONS} (rule-based)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keypoint Mapping (DogFLW 46 → Project 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEYPOINT_NAMES = [\n",
    "    \"left_eye\", \"right_eye\", \"nose\",\n",
    "    \"left_ear_base\", \"right_ear_base\", \"left_ear_tip\", \"right_ear_tip\",\n",
    "    \"left_mouth_corner\", \"right_mouth_corner\", \"upper_lip\", \"lower_lip\", \"chin\",\n",
    "    \"left_cheek\", \"right_cheek\", \"forehead\",\n",
    "    \"left_eyebrow\", \"right_eyebrow\",\n",
    "    \"muzzle_top\", \"muzzle_left\", \"muzzle_right\",\n",
    "]\n",
    "\n",
    "# DogFLW (46) → Project (20) mapping\n",
    "# Based on DogFLW landmark definitions\n",
    "DOGFLW_TO_PROJECT = {\n",
    "    0: 0,   # left_eye\n",
    "    1: 1,   # right_eye  \n",
    "    14: 2,  # nose\n",
    "    32: 3,  # left_ear_base (approximate)\n",
    "    36: 4,  # right_ear_base (approximate)\n",
    "    34: 5,  # left_ear_tip\n",
    "    38: 6,  # right_ear_tip\n",
    "    20: 7,  # left_mouth_corner\n",
    "    24: 8,  # right_mouth_corner\n",
    "    22: 9,  # upper_lip (mouth top)\n",
    "    26: 10, # lower_lip (mouth bottom)\n",
    "    28: 11, # chin\n",
    "    4: 12,  # left_cheek\n",
    "    8: 13,  # right_cheek\n",
    "    40: 14, # forehead (top of head)\n",
    "    42: 15, # left_eyebrow\n",
    "    44: 16, # right_eyebrow\n",
    "    16: 17, # muzzle_top\n",
    "    6: 18,  # muzzle_left\n",
    "    10: 19, # muzzle_right\n",
    "}\n",
    "\n",
    "PROJECT_TO_DOGFLW = {v: k for k, v in DOGFLW_TO_PROJECT.items()}\n",
    "\n",
    "def map_keypoints_46_to_20(kp46):\n",
    "    \"\"\"Конвертирует 46 DogFLW keypoints в 20 project keypoints.\"\"\"\n",
    "    kp20 = []\n",
    "    for i in range(NUM_KEYPOINTS_PROJECT):\n",
    "        dogflw_idx = PROJECT_TO_DOGFLW[i]\n",
    "        kp20.append(kp46[dogflw_idx])\n",
    "    return np.array(kp20)\n",
    "\n",
    "print(f'Keypoint mapping: {NUM_KEYPOINTS_DOGFLW} → {NUM_KEYPOINTS_PROJECT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ЭТАП 1: Обучение Keypoints модели\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dataset для Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DogFLWDataset(Dataset):\n",
    "    \"\"\"Dataset для DogFLW keypoints.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_path, split='train', image_size=256, heatmap_size=64):\n",
    "        self.image_size = image_size\n",
    "        self.heatmap_size = heatmap_size\n",
    "        self.sigma = 2.0  # Gaussian sigma for heatmaps\n",
    "        \n",
    "        images_dir = os.path.join(root_path, split, 'images')\n",
    "        labels_dir = os.path.join(root_path, split, 'labels')\n",
    "        \n",
    "        self.samples = []\n",
    "        for label_file in os.listdir(labels_dir):\n",
    "            if not label_file.endswith('.json'):\n",
    "                continue\n",
    "            \n",
    "            name = label_file.replace('.json', '')\n",
    "            # Try different image extensions\n",
    "            for ext in ['.jpg', '.jpeg', '.png', '.JPEG']:\n",
    "                img_path = os.path.join(images_dir, name + ext)\n",
    "                if os.path.exists(img_path):\n",
    "                    label_path = os.path.join(labels_dir, label_file)\n",
    "                    self.samples.append((img_path, label_path))\n",
    "                    break\n",
    "        \n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "        \n",
    "        self.transform_val = A.Compose([\n",
    "            A.Resize(image_size, image_size),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))\n",
    "        \n",
    "        self.is_train = (split == 'train')\n",
    "        print(f'{split}: {len(self.samples)} samples')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def _generate_heatmap(self, keypoints, h, w):\n",
    "        \"\"\"Генерирует heatmaps для keypoints.\"\"\"\n",
    "        num_kp = len(keypoints)\n",
    "        heatmaps = np.zeros((num_kp, self.heatmap_size, self.heatmap_size), dtype=np.float32)\n",
    "        \n",
    "        for i, (x, y) in enumerate(keypoints):\n",
    "            # Scale to heatmap size\n",
    "            x_hm = x * self.heatmap_size / w\n",
    "            y_hm = y * self.heatmap_size / h\n",
    "            \n",
    "            if 0 <= x_hm < self.heatmap_size and 0 <= y_hm < self.heatmap_size:\n",
    "                # Generate Gaussian\n",
    "                xx, yy = np.meshgrid(np.arange(self.heatmap_size), np.arange(self.heatmap_size))\n",
    "                heatmaps[i] = np.exp(-((xx - x_hm)**2 + (yy - y_hm)**2) / (2 * self.sigma**2))\n",
    "        \n",
    "        return heatmaps\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label_path = self.samples[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        h, w = image.shape[:2]\n",
    "        \n",
    "        # Load keypoints\n",
    "        with open(label_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        keypoints = [(float(x), float(y)) for x, y in data['landmarks']]\n",
    "        \n",
    "        # Apply transforms\n",
    "        transform = self.transform if self.is_train else self.transform_val\n",
    "        transformed = transform(image=image, keypoints=keypoints)\n",
    "        \n",
    "        image_tensor = transformed['image']\n",
    "        kp_transformed = transformed['keypoints']\n",
    "        \n",
    "        # Pad keypoints if some were removed\n",
    "        while len(kp_transformed) < NUM_KEYPOINTS_DOGFLW:\n",
    "            kp_transformed.append((0.0, 0.0))\n",
    "        \n",
    "        # Generate heatmaps\n",
    "        heatmaps = self._generate_heatmap(kp_transformed, self.image_size, self.image_size)\n",
    "        heatmaps_tensor = torch.from_numpy(heatmaps)\n",
    "        \n",
    "        return image_tensor, heatmaps_tensor\n",
    "\n",
    "# Test dataset\n",
    "train_kp_dataset = DogFLWDataset(DOGFLW_PATH, 'train')\n",
    "test_kp_dataset = DogFLWDataset(DOGFLW_PATH, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация примера\n",
    "img, hm = train_kp_dataset[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Image\n",
    "img_show = img.permute(1, 2, 0).numpy()\n",
    "img_show = img_show * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
    "img_show = np.clip(img_show, 0, 1)\n",
    "axes[0].imshow(img_show)\n",
    "axes[0].set_title('Image')\n",
    "\n",
    "# Heatmap sum\n",
    "axes[1].imshow(hm.sum(0).numpy(), cmap='hot')\n",
    "axes[1].set_title('Heatmaps (sum)')\n",
    "\n",
    "# Single heatmap\n",
    "axes[2].imshow(hm[0].numpy(), cmap='hot')\n",
    "axes[2].set_title('Keypoint 0 heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Keypoints Model (SimpleBaseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleBaselineNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Baseline for keypoint detection.\n",
    "    ResNet backbone + Deconv head → Heatmaps\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoints=46, backbone='resnet34'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone\n",
    "        self.backbone = timm.create_model(\n",
    "            backbone, \n",
    "            pretrained=True, \n",
    "            features_only=True,\n",
    "            out_indices=[-1]\n",
    "        )\n",
    "        \n",
    "        # Get backbone output channels\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, 3, 256, 256)\n",
    "            feat = self.backbone(dummy)[-1]\n",
    "            in_channels = feat.shape[1]\n",
    "        \n",
    "        # Deconv head\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.head = nn.Conv2d(256, num_keypoints, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)[-1]\n",
    "        x = self.deconv(features)\n",
    "        heatmaps = self.head(x)\n",
    "        return heatmaps\n",
    "\n",
    "# Create model\n",
    "kp_model = SimpleBaselineNet(NUM_KEYPOINTS_DOGFLW, 'resnet34').to(device)\n",
    "\n",
    "# Test forward pass\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 256, 256).to(device)\n",
    "    out = kp_model(dummy)\n",
    "    print(f'Input: {dummy.shape}')\n",
    "    print(f'Output: {out.shape}')\n",
    "    print(f'Parameters: {sum(p.numel() for p in kp_model.parameters()):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Обучение Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "train_kp_loader = DataLoader(train_kp_dataset, batch_size=BATCH_SIZE_KP, shuffle=True, num_workers=2, pin_memory=True)\n",
    "test_kp_loader = DataLoader(test_kp_dataset, batch_size=BATCH_SIZE_KP, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion_kp = nn.MSELoss()\n",
    "optimizer_kp = torch.optim.Adam(kp_model.parameters(), lr=LR_KP)\n",
    "scheduler_kp = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer_kp, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "print(f'Train batches: {len(train_kp_loader)}')\n",
    "print(f'Test batches: {len(test_kp_loader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_keypoints_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for images, heatmaps in tqdm(loader, desc='Train', leave=False):\n",
    "        images = images.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(images)\n",
    "        loss = criterion(pred, heatmaps)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)\n",
    "\n",
    "\n",
    "def eval_keypoints(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, heatmaps in tqdm(loader, desc='Eval', leave=False):\n",
    "            images = images.to(device)\n",
    "            heatmaps = heatmaps.to(device)\n",
    "            \n",
    "            pred = model(images)\n",
    "            loss = criterion(pred, heatmaps)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "print(f'\\n{\"=\"*60}')\n",
    "print(f'ЭТАП 1: Обучение Keypoints модели ({EPOCHS_KP} epochs)')\n",
    "print(f'{\"=\"*60}\\n')\n",
    "\n",
    "best_kp_loss = float('inf')\n",
    "kp_history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "for epoch in range(EPOCHS_KP):\n",
    "    train_loss = train_keypoints_epoch(kp_model, train_kp_loader, criterion_kp, optimizer_kp)\n",
    "    val_loss = eval_keypoints(kp_model, test_kp_loader, criterion_kp)\n",
    "    \n",
    "    scheduler_kp.step(val_loss)\n",
    "    \n",
    "    kp_history['train_loss'].append(train_loss)\n",
    "    kp_history['val_loss'].append(val_loss)\n",
    "    \n",
    "    if val_loss < best_kp_loss:\n",
    "        best_kp_loss = val_loss\n",
    "        torch.save(kp_model.state_dict(), 'keypoints_dogflw.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        lr = optimizer_kp.param_groups[0]['lr']\n",
    "        print(f'Epoch {epoch+1:3d}/{EPOCHS_KP} | Train: {train_loss:.6f} | Val: {val_loss:.6f} | Best: {best_kp_loss:.6f} | LR: {lr:.6f}')\n",
    "\n",
    "print(f'\\nKeypoints training done! Best loss: {best_kp_loss:.6f}')\n",
    "print('Saved: keypoints_dogflw.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot keypoints training\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(kp_history['train_loss'], label='Train')\n",
    "plt.plot(kp_history['val_loss'], label='Val')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.title('Keypoints Model Training')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('keypoints_training.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Визуализация Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "kp_model.load_state_dict(torch.load('keypoints_dogflw.pt'))\n",
    "kp_model.eval()\n",
    "\n",
    "def decode_heatmaps(heatmaps, original_size):\n",
    "    \"\"\"Декодирует heatmaps в координаты keypoints.\"\"\"\n",
    "    h, w = original_size\n",
    "    num_kp, hm_h, hm_w = heatmaps.shape\n",
    "    \n",
    "    keypoints = []\n",
    "    confidences = []\n",
    "    \n",
    "    for k in range(num_kp):\n",
    "        hm = heatmaps[k]\n",
    "        idx = hm.argmax()\n",
    "        y_hm, x_hm = idx // hm_w, idx % hm_w\n",
    "        conf = float(hm.max())\n",
    "        \n",
    "        x = float(x_hm) * w / hm_w\n",
    "        y = float(y_hm) * h / hm_h\n",
    "        \n",
    "        keypoints.append((x, y))\n",
    "        confidences.append(conf)\n",
    "    \n",
    "    return keypoints, confidences\n",
    "\n",
    "# Test on sample\n",
    "img_tensor, gt_hm = test_kp_dataset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_hm = kp_model(img_tensor.unsqueeze(0).to(device))[0].cpu().numpy()\n",
    "\n",
    "kp_pred, conf = decode_heatmaps(pred_hm, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "img_show = img_tensor.permute(1, 2, 0).numpy()\n",
    "img_show = img_show * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n",
    "img_show = np.clip(img_show, 0, 1)\n",
    "\n",
    "# Image with keypoints\n",
    "axes[0].imshow(img_show)\n",
    "for i, ((x, y), c) in enumerate(zip(kp_pred, conf)):\n",
    "    if c > 0.1:\n",
    "        axes[0].plot(x, y, 'ro', markersize=3)\n",
    "axes[0].set_title('Predicted Keypoints')\n",
    "\n",
    "# GT heatmaps\n",
    "axes[1].imshow(gt_hm.sum(0).numpy(), cmap='hot')\n",
    "axes[1].set_title('GT Heatmaps')\n",
    "\n",
    "# Pred heatmaps\n",
    "axes[2].imshow(pred_hm.sum(0), cmap='hot')\n",
    "axes[2].set_title('Pred Heatmaps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('keypoints_prediction.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n# ETAP 2: Rule-Based Emotion Classification (bez treningu!)\n---\n\nKlasyfikacja emocji oparta na naukowych wagach z DogFACS (Mota-Rojas et al. 2021).\n\n**Nie wymaga treningu** - używa wzorów:\n- happy = AU_mouth_open * 0.35 + AU_ear_forward * 0.25 + ...\n- sad = AU_ear_back * 0.40 + AU_eye_blink * 0.15 + ...\n- angry = (AU_mouth_open + AU_jaw_drop)/2 * 0.30 + AU_lip_pull * 0.25 + ...\n- fearful = AU_ear_back * 0.30 + AU_nose_lick * 0.25 + ...\n- relaxed = (1 - mean_activation) * 0.50 + ...\n- neutral = (1 - mean_activation) * 0.70 + ..."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.1 Rule-Based Emotion Classifier"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Keypoint indices for AU computation\nKP = {name: i for i, name in enumerate(KEYPOINT_NAMES)}\n\nACTION_UNIT_NAMES = [\n    \"AU_brow_raise\", \"AU_ear_forward\", \"AU_ear_back\", \"AU_ear_asymmetry\",\n    \"AU_eye_opening\", \"AU_mouth_open\", \"AU_lip_corner_pull\", \"AU_jaw_drop\",\n    \"AU_nose_wrinkle\", \"AU_muzzle_width\", \"AU_face_elongation\", \"AU_eye_distance\",\n]\n\ndef extract_action_units(keypoints_flat):\n    \"\"\"\n    Wyodrębnia 12 Action Units z 60 keypoints features.\n    keypoints_flat: [x0, y0, v0, x1, y1, v1, ...] (60 values)\n    \"\"\"\n    kp = keypoints_flat.reshape(20, 3)\n    coords = kp[:, :2]\n    vis = kp[:, 2]\n    \n    def dist(i, j):\n        return float(np.sqrt(np.sum((coords[i] - coords[j]) ** 2)))\n    \n    def ang(i, j):\n        d = coords[j] - coords[i]\n        return math.atan2(d[1], d[0])\n    \n    # Reference distance (between eyes)\n    eye_d = max(dist(KP['left_eye'], KP['right_eye']), 1e-6)\n    \n    # Compute Action Units\n    brow_d = (dist(KP['left_eyebrow'], KP['left_eye']) + dist(KP['right_eyebrow'], KP['right_eye'])) / 2\n    au_brow = np.clip((brow_d / eye_d - 0.2) / 0.3, 0, 1)\n    \n    l_ear = ang(KP['left_ear_base'], KP['left_ear_tip'])\n    r_ear = ang(KP['right_ear_base'], KP['right_ear_tip'])\n    au_ear_fwd = np.clip(1.0 - (abs(l_ear) + abs(r_ear)) / 2 / math.pi, 0, 1)\n    au_ear_back = 1.0 - au_ear_fwd\n    au_ear_asym = np.clip(abs(l_ear - r_ear) / math.pi, 0, 1)\n    \n    au_eye = np.clip((vis[KP['left_eye']] + vis[KP['right_eye']]) / 2, 0, 1)\n    au_mouth = np.clip(dist(KP['upper_lip'], KP['lower_lip']) / eye_d / 0.3, 0, 1)\n    \n    l_lip = ang(KP['upper_lip'], KP['left_mouth_corner'])\n    r_lip = ang(KP['upper_lip'], KP['right_mouth_corner'])\n    au_smile = np.clip((l_lip - r_lip + math.pi) / (2 * math.pi), 0, 1)\n    \n    au_jaw = np.clip((dist(KP['nose'], KP['chin']) / eye_d - 0.5), 0, 1)\n    au_nose = np.clip(1.0 - dist(KP['nose'], KP['upper_lip']) / eye_d / 0.5, 0, 1)\n    au_muzzle = np.clip((dist(KP['muzzle_left'], KP['muzzle_right']) / eye_d - 0.3) / 0.5, 0, 1)\n    au_elong = np.clip((dist(KP['forehead'], KP['chin']) / eye_d - 1.0) / 1.5, 0, 1)\n    au_eye_d = np.clip(eye_d / 100.0, 0, 1)\n    \n    return {\n        'AU_brow_raise': float(au_brow),\n        'AU_ear_forward': float(au_ear_fwd),\n        'AU_ear_back': float(au_ear_back),\n        'AU_ear_asymmetry': float(au_ear_asym),\n        'AU_eye_opening': float(au_eye),\n        'AU_mouth_open': float(au_mouth),\n        'AU_lip_corner_pull': float(au_smile),\n        'AU_jaw_drop': float(au_jaw),\n        'AU_nose_wrinkle': float(au_nose),\n        'AU_muzzle_width': float(au_muzzle),\n        'AU_face_elongation': float(au_elong),\n        'AU_eye_distance': float(au_eye_d),\n    }\n\n\ndef classify_emotion_rule_based(au_values, neutral_threshold=0.35):\n    \"\"\"\n    Rule-based klasyfikacja emocji na podstawie Action Units.\n    Oparta na DogFACS (Mota-Rojas et al. 2021).\n    \n    NIE WYMAGA TRENINGU - używa naukowych wag.\n    \"\"\"\n    # Get AU values\n    brow_raise = au_values.get('AU_brow_raise', 0.0)\n    eye_opening = au_values.get('AU_eye_opening', 0.5)\n    mouth_open = au_values.get('AU_mouth_open', 0.0)\n    jaw_drop = au_values.get('AU_jaw_drop', 0.0)\n    nose_wrinkle = au_values.get('AU_nose_wrinkle', 0.0)\n    lip_corner_pull = au_values.get('AU_lip_corner_pull', 0.0)\n    ear_forward = au_values.get('AU_ear_forward', 0.0)\n    ear_back = au_values.get('AU_ear_back', 0.0)\n    ear_asymmetry = au_values.get('AU_ear_asymmetry', 0.0)\n    \n    # Derived values\n    blink = 1.0 - eye_opening\n    nose_lick = max(0.0, mouth_open * 0.5 - jaw_drop * 0.3)  # Proxy\n    \n    all_au = [brow_raise, blink, mouth_open, jaw_drop, nose_wrinkle,\n              lip_corner_pull, ear_forward, ear_back, ear_asymmetry, nose_lick]\n    mean_activation = sum(all_au) / len(all_au)\n    \n    # === EMOTION SCORING (wg QUICK_IMPLEMENTATION_PLAN.md) ===\n    \n    happy_score = (\n        mouth_open * 0.35 + ear_forward * 0.25 + brow_raise * 0.15 +\n        (1 - ear_back) * 0.15 + (1 - nose_lick) * 0.10\n    )\n    \n    sad_score = (\n        ear_back * 0.40 + blink * 0.15 + (1 - brow_raise) * 0.15 +\n        (1 - mouth_open) * 0.15 + nose_lick * 0.15\n    )\n    \n    angry_score = (\n        ((mouth_open + jaw_drop) / 2) * 0.30 + lip_corner_pull * 0.25 +\n        nose_wrinkle * 0.15 + ((ear_back + ear_asymmetry) / 2) * 0.15 + blink * 0.15\n    )\n    \n    fearful_score = (\n        ear_back * 0.30 + nose_lick * 0.25 + blink * 0.20 +\n        brow_raise * 0.12 + (1 - mouth_open) * 0.13\n    )\n    \n    relaxed_score = (\n        (1 - mean_activation) * 0.50 + (1 - ear_back) * 0.15 +\n        (1 - ear_forward) * 0.15 + (1 - nose_lick) * 0.10 + (1 - nose_wrinkle) * 0.10\n    )\n    \n    neutral_score = (\n        (1 - mean_activation) * 0.70 +\n        (1 - (mouth_open + jaw_drop) / 2) * 0.15 +\n        (1 - (ear_back + ear_forward) / 2) * 0.15\n    )\n    \n    scores = {\n        'happy': happy_score,\n        'sad': sad_score,\n        'angry': angry_score,\n        'fearful': fearful_score,\n        'relaxed': relaxed_score,\n        'neutral': neutral_score,\n    }\n    \n    # Softmax-like normalization\n    temperature = 2.0\n    exp_scores = {k: np.exp(v * temperature) for k, v in scores.items()}\n    total_exp = sum(exp_scores.values())\n    probabilities = {k: v / total_exp for k, v in exp_scores.items()}\n    \n    # Find best emotion\n    best_emotion = max(probabilities, key=probabilities.get)\n    best_prob = probabilities[best_emotion]\n    \n    # Threshold for neutral\n    if best_prob < neutral_threshold and best_emotion != 'neutral':\n        best_emotion = 'neutral'\n        best_prob = probabilities['neutral']\n    \n    return {\n        'emotion': best_emotion,\n        'confidence': best_prob,\n        'probabilities': probabilities,\n        'action_units': au_values,\n    }\n\nprint(f'Action Units: {len(ACTION_UNIT_NAMES)}')\nprint(f'Rule-based classifier ready!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.2 Full Pipeline Demo"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class FullEmotionPipeline:\n    \"\"\"\n    Pełny pipeline: Image → Keypoints → AU → Emotion\n    \n    Używa:\n    - Wytrenowanego modelu keypoints (SimpleBaseline)\n    - Ekstrakcji Action Units z geometrii\n    - Rule-based klasyfikacji emocji (DogFACS)\n    \"\"\"\n    \n    def __init__(self, kp_model, device):\n        self.kp_model = kp_model\n        self.kp_model.eval()\n        self.device = device\n        \n        self.transform = transforms.Compose([\n            transforms.ToPILImage(),\n            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    \n    def extract_keypoints(self, image):\n        \"\"\"Wyodrębnia 20 keypoints z obrazu.\"\"\"\n        if isinstance(image, str):\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        h, w = image.shape[:2]\n        tensor = self.transform(image).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            heatmaps = self.kp_model(tensor)[0].cpu().numpy()\n        \n        # Decode 46 keypoints\n        hm_h, hm_w = heatmaps.shape[1:]\n        kp46 = []\n        for k in range(NUM_KEYPOINTS_DOGFLW):\n            hm = heatmaps[k]\n            idx = hm.argmax()\n            y_hm, x_hm = idx // hm_w, idx % hm_w\n            conf = float(hm.max())\n            x = x_hm * w / hm_w\n            y = y_hm * h / hm_h\n            kp46.append((x, y, conf))\n        \n        # Map 46 → 20\n        result = []\n        for i in range(NUM_KEYPOINTS_PROJECT):\n            dogflw_idx = PROJECT_TO_DOGFLW[i]\n            x, y, v = kp46[dogflw_idx]\n            result.extend([x, y, v])\n        \n        return np.array(result, dtype=np.float32)\n    \n    def predict(self, image):\n        \"\"\"\n        Pełna predykcja: Image → Emotion\n        \n        Returns:\n            dict z emotion, confidence, probabilities, action_units\n        \"\"\"\n        # 1. Extract keypoints\n        keypoints = self.extract_keypoints(image)\n        \n        # 2. Compute Action Units\n        au_values = extract_action_units(keypoints)\n        \n        # 3. Rule-based classification\n        result = classify_emotion_rule_based(au_values)\n        \n        # Add keypoints to result\n        result['keypoints'] = keypoints\n        \n        return result\n\n# Create pipeline with trained model\npipeline = FullEmotionPipeline(kp_model, device)\nprint('Full Emotion Pipeline ready!')\nprint('  Image → Keypoints → Action Units → Emotion')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.3 Demo na przykładowych obrazach"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Demo na kilku obrazach z test set\ntest_images_dir = os.path.join(DOGFLW_PATH, 'test', 'images')\ntest_images = os.listdir(test_images_dir)[:5]  # Pierwsze 5\n\nfig, axes = plt.subplots(1, 5, figsize=(20, 4))\n\nfor i, img_name in enumerate(test_images):\n    img_path = os.path.join(test_images_dir, img_name)\n    \n    # Load image\n    img = cv2.imread(img_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Predict\n    result = pipeline.predict(img_rgb)\n    \n    # Draw keypoints\n    kp = result['keypoints'].reshape(20, 3)\n    for x, y, v in kp:\n        if v > 0.1:\n            cv2.circle(img_rgb, (int(x), int(y)), 3, (255, 0, 0), -1)\n    \n    # Display\n    axes[i].imshow(img_rgb)\n    axes[i].set_title(f\"{result['emotion']}\\n{result['confidence']:.1%}\")\n    axes[i].axis('off')\n\nplt.tight_layout()\nplt.savefig('emotion_demo.png', dpi=150)\nplt.show()\n\n# Show AU values for last image\nprint('\\nAction Units dla ostatniego obrazu:')\nfor name, value in result['action_units'].items():\n    print(f'  {name}: {value:.3f}')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Probability distribution dla ostatniego obrazu\nprobs = result['probabilities']\n\nplt.figure(figsize=(10, 4))\ncolors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12', '#95a5a6']\nplt.bar(probs.keys(), probs.values(), color=colors)\nplt.xlabel('Emotion')\nplt.ylabel('Probability')\nplt.title('Emotion Probabilities (Rule-Based)')\nplt.ylim(0, 1)\nplt.savefig('emotion_probabilities.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2.4 Statystyki na całym test set"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analiza emocji na całym test set\ntest_images_dir = os.path.join(DOGFLW_PATH, 'test', 'images')\nall_test_images = [f for f in os.listdir(test_images_dir) if f.endswith(('.jpg', '.jpeg', '.png', '.JPEG'))]\n\nemotion_counts = {e: 0 for e in EMOTION_CLASSES}\nall_confidences = []\n\nprint(f'Analizuję {len(all_test_images)} obrazów...')\n\nfor img_name in tqdm(all_test_images):\n    img_path = os.path.join(test_images_dir, img_name)\n    try:\n        img = cv2.imread(img_path)\n        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        result = pipeline.predict(img_rgb)\n        \n        emotion_counts[result['emotion']] += 1\n        all_confidences.append(result['confidence'])\n    except:\n        pass\n\nprint('\\nRozkład emocji na test set:')\nfor emotion, count in emotion_counts.items():\n    pct = 100 * count / len(all_test_images) if all_test_images else 0\n    print(f'  {emotion}: {count} ({pct:.1f}%)')\n\nprint(f'\\nŚrednia pewność: {np.mean(all_confidences):.1%}')"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Wizualizacja rozkładu emocji\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Emotion distribution\ncolors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12', '#95a5a6']\nax1.bar(emotion_counts.keys(), emotion_counts.values(), color=colors)\nax1.set_xlabel('Emotion')\nax1.set_ylabel('Count')\nax1.set_title('Emotion Distribution (Rule-Based on Test Set)')\n\n# Confidence histogram\nax2.hist(all_confidences, bins=20, color='steelblue', edgecolor='black', alpha=0.7)\nax2.axvline(x=np.mean(all_confidences), color='red', linestyle='--', label=f'Mean: {np.mean(all_confidences):.2f}')\nax2.set_xlabel('Confidence')\nax2.set_ylabel('Count')\nax2.set_title('Confidence Distribution')\nax2.legend()\n\nplt.tight_layout()\nplt.savefig('test_set_analysis.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "---\n# Zapisanie wyników\n---"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Zapisz metryki (handle inf for JSON)\nmetrics = {\n    'approach': 'rule-based',\n    'keypoints': {\n        'best_loss': float(best_kp_loss) if best_kp_loss != float('inf') else None,\n        'epochs': EPOCHS_KP,\n        'num_keypoints_dogflw': NUM_KEYPOINTS_DOGFLW,\n        'num_keypoints_project': NUM_KEYPOINTS_PROJECT,\n    },\n    'action_units': {\n        'count': NUM_ACTION_UNITS,\n        'names': ACTION_UNIT_NAMES,\n    },\n    'emotion': {\n        'method': 'DogFACS rule-based (no training required)',\n        'num_classes': NUM_EMOTIONS,\n        'classes': EMOTION_CLASSES,\n        'neutral_threshold': NEUTRAL_THRESHOLD,\n        'test_distribution': emotion_counts,\n        'mean_confidence': float(np.mean(all_confidences)),\n    },\n    'kp_history': kp_history,\n}\n\nwith open('training_metrics.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\n\nprint('Zapisane pliki:')\nprint('  ✓ keypoints_dogflw.pt (model keypoints)')\nprint('  ✓ training_metrics.json (metryki)')\nprint('  ✓ *.png (wykresy)')\nprint()\nprint('UWAGA: Model emocji NIE jest potrzebny!')\nprint('       Klasyfikacja emocji jest rule-based.')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Weryfikacja modelu\nprint('=== Weryfikacja ===\\n')\n\n# Test keypoints model\ntest_kp = SimpleBaselineNet(46, 'resnet34')\ntest_kp.load_state_dict(torch.load('keypoints_dogflw.pt'))\ntest_kp.eval()\n\nwith torch.no_grad():\n    out = test_kp(torch.randn(1, 3, 256, 256))\nprint(f'Keypoints Model:')\nprint(f'  Input: (1, 3, 256, 256)')\nprint(f'  Output: {tuple(out.shape)}')\nprint()\n\n# Test full pipeline\nprint('Full Pipeline:')\ndummy_kp = np.random.randn(60).astype(np.float32)  # 20 keypoints * 3\nau = extract_action_units(dummy_kp)\nresult = classify_emotion_rule_based(au)\nprint(f'  Keypoints → AU → Emotion')\nprint(f'  Input: 60 keypoint values')\nprint(f'  AU: {len(au)} action units')\nprint(f'  Output: {result[\"emotion\"]} ({result[\"confidence\"]:.1%})')\nprint()\nprint('✓ Wszystko działa!')"
  },
  {
   "cell_type": "markdown",
   "source": "---\n## Po zakończeniu treningu\n\n**Pobierz plik:**\n- `keypoints_dogflw.pt` → skopiuj do `models/keypoints_dogflw.pt`\n\n**Oczekiwane wyniki:**\n\n| Model | Metryka | Wartość |\n|-------|---------|---------|\n| Keypoints | MSE Loss | < 0.01 |\n| Emotion | Metoda | Rule-based (bez treningu) |\n\n**Klasyfikacja emocji:**\n- Używa DogFACS (Mota-Rojas et al. 2021)\n- Nie wymaga treningu\n- 6 klas: happy, sad, angry, fearful, relaxed, neutral\n- Neutral gdy `max(prob) < 0.35`\n\n**Zalety podejścia rule-based:**\n1. Interpretowalność - wiemy dokładnie dlaczego model przewiduje daną emocję\n2. Brak potrzeby danych treningowych dla emocji\n3. Oparte na naukowych badaniach DogFACS\n4. Łatwe do modyfikacji i dostrajania wag",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}