{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dog Emotion Classification - Training on Keypoints\n",
    "\n",
    "Trening modelu klasyfikacji emocji psów na podstawie 20 keypoints twarzy.\n",
    "\n",
    "**Architektura:**\n",
    "- Input: 60 wartości (20 keypoints × 3: x, y, visibility)\n",
    "- Model: MLP (256 → 128 → 64 → 6)\n",
    "- Output: 6 klas emocji (happy, sad, angry, fearful, relaxed, neutral)\n",
    "\n",
    "**Wymagane datasety Kaggle:**\n",
    "1. `lovodkin/dogflw` - keypoints twarzy psów (46 punktów → mapujemy do 20)\n",
    "2. `dougandrade/dog-emotions-5-classes` - obrazy z etykietami emocji\n",
    "\n",
    "**Po zakończeniu:**\n",
    "Pobierz plik `emotion_keypoints.pt` i przekaż go do projektu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja zależności\n",
    "!pip install -q timm albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === KONFIGURACJA ===\n",
    "\n",
    "# Ścieżki do datasetów (dostosuj do Kaggle)\n",
    "DOGFLW_PATH = '/kaggle/input/dogflw'  # Dataset z keypoints\n",
    "EMOTIONS_PATH = '/kaggle/input/dog-emotions-5-classes'  # Dataset z emocjami\n",
    "\n",
    "# Parametry modelu\n",
    "NUM_KEYPOINTS = 20\n",
    "INPUT_FEATURES = NUM_KEYPOINTS * 3  # 60\n",
    "NUM_EMOTIONS = 6\n",
    "EMOTION_CLASSES = ['happy', 'sad', 'angry', 'fearful', 'relaxed', 'neutral']\n",
    "\n",
    "# Mapping emocji z datasetu 5-klasowego do naszego 6-klasowego\n",
    "# Dataset 5-class: happy, sad, angry, fearful, relaxed (brak neutral)\n",
    "EMOTION_MAPPING = {\n",
    "    'happy': 0,\n",
    "    'sad': 1,\n",
    "    'angry': 2,\n",
    "    'fear': 3,      # W datasecie może być 'fear' zamiast 'fearful'\n",
    "    'fearful': 3,\n",
    "    'relaxed': 4,\n",
    "    'neutral': 5,\n",
    "}\n",
    "\n",
    "# Parametry treningu\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "HIDDEN_DIMS = [256, 128, 64]\n",
    "DROPOUT = 0.3\n",
    "\n",
    "print(f'Input features: {INPUT_FEATURES}')\n",
    "print(f'Emotion classes: {EMOTION_CLASSES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mapping DogFLW (46) → Project (20) Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwy 20 keypoints projektu\n",
    "KEYPOINT_NAMES = [\n",
    "    \"left_eye\",           # 0\n",
    "    \"right_eye\",          # 1\n",
    "    \"nose\",               # 2\n",
    "    \"left_ear_base\",      # 3\n",
    "    \"right_ear_base\",     # 4\n",
    "    \"left_ear_tip\",       # 5\n",
    "    \"right_ear_tip\",      # 6\n",
    "    \"left_mouth_corner\",  # 7\n",
    "    \"right_mouth_corner\", # 8\n",
    "    \"upper_lip\",          # 9\n",
    "    \"lower_lip\",          # 10\n",
    "    \"chin\",               # 11\n",
    "    \"left_cheek\",         # 12\n",
    "    \"right_cheek\",        # 13\n",
    "    \"forehead\",           # 14\n",
    "    \"left_eyebrow\",       # 15\n",
    "    \"right_eyebrow\",      # 16\n",
    "    \"muzzle_top\",         # 17\n",
    "    \"muzzle_left\",        # 18\n",
    "    \"muzzle_right\",       # 19\n",
    "]\n",
    "\n",
    "# Mapping: DogFLW index → Project index\n",
    "DOGFLW_TO_PROJECT = {\n",
    "    0: 0,   # left_eye\n",
    "    1: 1,   # right_eye\n",
    "    14: 2,  # nose\n",
    "    32: 3,  # left_ear_base\n",
    "    36: 4,  # right_ear_base\n",
    "    34: 5,  # left_ear_tip\n",
    "    38: 6,  # right_ear_tip\n",
    "    20: 7,  # left_mouth_corner\n",
    "    24: 8,  # right_mouth_corner\n",
    "    22: 9,  # upper_lip\n",
    "    26: 10, # lower_lip\n",
    "    28: 11, # chin\n",
    "    4: 12,  # left_cheek\n",
    "    8: 13,  # right_cheek\n",
    "    40: 14, # forehead\n",
    "    42: 15, # left_eyebrow\n",
    "    44: 16, # right_eyebrow\n",
    "    16: 17, # muzzle_top\n",
    "    6: 18,  # muzzle_left\n",
    "    10: 19, # muzzle_right\n",
    "}\n",
    "\n",
    "# Odwrotny mapping: Project index → DogFLW index\n",
    "PROJECT_TO_DOGFLW = {v: k for k, v in DOGFLW_TO_PROJECT.items()}\n",
    "\n",
    "print(f'Mapping zdefiniowany: {len(PROJECT_TO_DOGFLW)} keypoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Keypoints (do ekstrakcji z obrazów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "class SimpleBaselineModel(nn.Module):\n",
    "    \"\"\"Model do detekcji 46 keypoints DogFLW.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoints=46):\n",
    "        super().__init__()\n",
    "        self.bb = timm.create_model(\n",
    "            'resnet50',\n",
    "            pretrained=False,\n",
    "            features_only=True,\n",
    "            out_indices=[-1],\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.out = nn.Conv2d(256, num_keypoints, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bb(x)[-1]\n",
    "        x = self.head(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class KeypointsExtractor:\n",
    "    \"\"\"Ekstraktor keypoints z obrazów.\"\"\"\n",
    "    \n",
    "    def __init__(self, weights_path=None):\n",
    "        self.model = SimpleBaselineModel(num_keypoints=46)\n",
    "        self.device = device\n",
    "        \n",
    "        if weights_path and os.path.exists(weights_path):\n",
    "            state_dict = torch.load(weights_path, map_location=device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            print(f'Loaded keypoints weights: {weights_path}')\n",
    "        else:\n",
    "            print('WARNING: No keypoints weights loaded!')\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def extract(self, image):\n",
    "        \"\"\"Ekstrahuje 20 keypoints z obrazu.\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = cv2.imread(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            heatmaps = self.model(tensor)[0]  # (46, H, W)\n",
    "        \n",
    "        # Dekoduj 46 keypoints\n",
    "        dogflw_keypoints = self._decode_heatmaps(heatmaps, w, h)\n",
    "        \n",
    "        # Mapuj do 20 keypoints\n",
    "        project_keypoints = self._map_to_project(dogflw_keypoints)\n",
    "        \n",
    "        return project_keypoints\n",
    "    \n",
    "    def _decode_heatmaps(self, heatmaps, target_w, target_h):\n",
    "        \"\"\"Dekoduje heatmapy do współrzędnych.\"\"\"\n",
    "        hm_h, hm_w = heatmaps.shape[1], heatmaps.shape[2]\n",
    "        scale_x = target_w / hm_w\n",
    "        scale_y = target_h / hm_h\n",
    "        \n",
    "        keypoints = []\n",
    "        for k in range(46):\n",
    "            hm = heatmaps[k].cpu().numpy()\n",
    "            max_val = hm.max()\n",
    "            max_idx = hm.argmax()\n",
    "            y_hm = max_idx // hm_w\n",
    "            x_hm = max_idx % hm_w\n",
    "            \n",
    "            x = float(x_hm * scale_x)\n",
    "            y = float(y_hm * scale_y)\n",
    "            visibility = float(max_val)\n",
    "            \n",
    "            keypoints.append((x, y, visibility))\n",
    "        \n",
    "        return keypoints\n",
    "    \n",
    "    def _map_to_project(self, dogflw_keypoints):\n",
    "        \"\"\"Mapuje 46 → 20 keypoints.\"\"\"\n",
    "        project_keypoints = []\n",
    "        for proj_idx in range(20):\n",
    "            dogflw_idx = PROJECT_TO_DOGFLW[proj_idx]\n",
    "            project_keypoints.append(dogflw_keypoints[dogflw_idx])\n",
    "        return project_keypoints\n",
    "\n",
    "\n",
    "print('KeypointsExtractor defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Emocji (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointsEmotionMLP(nn.Module):\n",
    "    \"\"\"MLP do klasyfikacji emocji na podstawie keypoints.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=INPUT_FEATURES,\n",
    "        hidden_dims=None,\n",
    "        num_classes=NUM_EMOTIONS,\n",
    "        dropout=0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [256, 128, 64]\n",
    "        \n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(prev_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout),\n",
    "            ])\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, num_classes))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "\n",
    "# Test modelu\n",
    "model = KeypointsEmotionMLP(\n",
    "    input_dim=INPUT_FEATURES,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    num_classes=NUM_EMOTIONS,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "print(f'Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Przygotowanie danych\n",
    "\n",
    "**Opcja A:** Użyj gotowych keypoints z DogFLW + etykiet emocji\n",
    "\n",
    "**Opcja B:** Ekstrahuj keypoints z obrazów emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź dostępne datasety\n",
    "print('=== Dostępne datasety ===')\n",
    "\n",
    "if os.path.exists(DOGFLW_PATH):\n",
    "    print(f'✓ DogFLW: {DOGFLW_PATH}')\n",
    "    print(f'  Files: {os.listdir(DOGFLW_PATH)[:5]}...')\n",
    "else:\n",
    "    print(f'✗ DogFLW not found at {DOGFLW_PATH}')\n",
    "\n",
    "if os.path.exists(EMOTIONS_PATH):\n",
    "    print(f'✓ Emotions: {EMOTIONS_PATH}')\n",
    "    print(f'  Files: {os.listdir(EMOTIONS_PATH)[:5]}...')\n",
    "else:\n",
    "    print(f'✗ Emotions not found at {EMOTIONS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPCJA A: Załaduj DogFLW keypoints ===\n",
    "# DogFLW zawiera keypoints, ale potrzebujemy etykiet emocji\n",
    "\n",
    "def load_dogflw_keypoints(dogflw_path):\n",
    "    \"\"\"Ładuje keypoints z DogFLW dataset.\"\"\"\n",
    "    # Szukaj pliku z anotacjami\n",
    "    for filename in ['landmarks.csv', 'annotations.csv', 'keypoints.csv']:\n",
    "        filepath = os.path.join(dogflw_path, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f'Loaded: {filepath}')\n",
    "            print(f'Columns: {list(df.columns)[:10]}...')\n",
    "            return df\n",
    "    \n",
    "    print('No keypoints CSV found in DogFLW')\n",
    "    return None\n",
    "\n",
    "# dogflw_df = load_dogflw_keypoints(DOGFLW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPCJA B: Ekstrahuj keypoints z emotion dataset ===\n",
    "\n",
    "def prepare_emotion_dataset(emotions_path, keypoints_extractor=None):\n",
    "    \"\"\"\n",
    "    Przygotowuje dataset z emotion images.\n",
    "    \n",
    "    Struktura oczekiwana:\n",
    "    emotions_path/\n",
    "        happy/\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "        sad/\n",
    "            ...\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for emotion_folder in os.listdir(emotions_path):\n",
    "        folder_path = os.path.join(emotions_path, emotion_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        emotion_name = emotion_folder.lower()\n",
    "        if emotion_name not in EMOTION_MAPPING:\n",
    "            print(f'Skipping unknown emotion: {emotion_name}')\n",
    "            continue\n",
    "        \n",
    "        emotion_id = EMOTION_MAPPING[emotion_name]\n",
    "        \n",
    "        images = [f for f in os.listdir(folder_path) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f'{emotion_name}: {len(images)} images')\n",
    "        \n",
    "        for img_name in tqdm(images, desc=emotion_name):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            if keypoints_extractor:\n",
    "                try:\n",
    "                    keypoints = keypoints_extractor.extract(img_path)\n",
    "                    # Flatten: [(x0,y0,v0), (x1,y1,v1), ...] → [x0,y0,v0,x1,y1,v1,...]\n",
    "                    flat_keypoints = []\n",
    "                    for x, y, v in keypoints:\n",
    "                        flat_keypoints.extend([x, y, v])\n",
    "                    \n",
    "                    data.append({\n",
    "                        'image_path': img_path,\n",
    "                        'emotion_id': emotion_id,\n",
    "                        'emotion_name': emotion_name,\n",
    "                        'keypoints': flat_keypoints,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing {img_path}: {e}')\n",
    "            else:\n",
    "                data.append({\n",
    "                    'image_path': img_path,\n",
    "                    'emotion_id': emotion_id,\n",
    "                    'emotion_name': emotion_name,\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print('prepare_emotion_dataset() defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Załaduj/Przygotuj dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj model keypoints (jeśli masz wagi)\n",
    "KEYPOINTS_WEIGHTS = '/kaggle/input/dogflw/keypoints_best.pt'  # Dostosuj ścieżkę\n",
    "\n",
    "if os.path.exists(KEYPOINTS_WEIGHTS):\n",
    "    extractor = KeypointsExtractor(KEYPOINTS_WEIGHTS)\n",
    "else:\n",
    "    print('Keypoints weights not found. Using None (will generate synthetic data)')\n",
    "    extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotuj dataset\n",
    "if extractor and os.path.exists(EMOTIONS_PATH):\n",
    "    print('Extracting keypoints from emotion images...')\n",
    "    df = prepare_emotion_dataset(EMOTIONS_PATH, extractor)\n",
    "    \n",
    "    # Zapisz do CSV dla przyszłego użycia\n",
    "    df.to_csv('emotion_keypoints_dataset.csv', index=False)\n",
    "    print(f'Dataset saved: emotion_keypoints_dataset.csv ({len(df)} samples)')\n",
    "else:\n",
    "    print('Using synthetic data for testing...')\n",
    "    # Generuj syntetyczne dane\n",
    "    n_samples = 5000\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    synthetic_keypoints = np.random.randn(n_samples, INPUT_FEATURES).astype(np.float32)\n",
    "    synthetic_labels = np.random.randint(0, NUM_EMOTIONS, n_samples)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'emotion_id': synthetic_labels,\n",
    "        'keypoints': [list(kp) for kp in synthetic_keypoints],\n",
    "    })\n",
    "    print(f'Synthetic dataset: {len(df)} samples')\n",
    "\n",
    "print(f'\\nDataset shape: {df.shape}')\n",
    "print(f'Emotion distribution:\\n{df[\"emotion_id\"].value_counts().sort_index()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset i DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionKeypointsDataset(Dataset):\n",
    "    \"\"\"Dataset dla treningu emotion classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        \n",
    "        # Przygotuj keypoints jako numpy array\n",
    "        if isinstance(self.df['keypoints'].iloc[0], str):\n",
    "            # Jeśli keypoints są zapisane jako string\n",
    "            self.keypoints = np.array([\n",
    "                eval(kp) for kp in self.df['keypoints']\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            self.keypoints = np.array(\n",
    "                self.df['keypoints'].tolist(), dtype=np.float32\n",
    "            )\n",
    "        \n",
    "        self.labels = self.df['emotion_id'].values.astype(np.int64)\n",
    "        \n",
    "        # Normalizacja keypoints\n",
    "        self.keypoints = self._normalize(self.keypoints)\n",
    "    \n",
    "    def _normalize(self, keypoints):\n",
    "        \"\"\"Normalizuje keypoints.\"\"\"\n",
    "        # Dla każdej próbki, normalizuj x,y względem zakresu\n",
    "        normalized = keypoints.copy()\n",
    "        \n",
    "        for i in range(len(normalized)):\n",
    "            kp = normalized[i]\n",
    "            # x: indices 0, 3, 6, ...\n",
    "            # y: indices 1, 4, 7, ...\n",
    "            # v: indices 2, 5, 8, ...\n",
    "            xs = kp[0::3]\n",
    "            ys = kp[1::3]\n",
    "            \n",
    "            # Normalizuj do [-1, 1]\n",
    "            if xs.max() > xs.min():\n",
    "                xs = 2 * (xs - xs.min()) / (xs.max() - xs.min()) - 1\n",
    "            if ys.max() > ys.min():\n",
    "                ys = 2 * (ys - ys.min()) / (ys.max() - ys.min()) - 1\n",
    "            \n",
    "            kp[0::3] = xs\n",
    "            kp[1::3] = ys\n",
    "            # visibility pozostaje bez zmian\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.keypoints[idx]),\n",
    "            torch.tensor(self.labels[idx]),\n",
    "        )\n",
    "\n",
    "\n",
    "# Stwórz dataset\n",
    "full_dataset = EmotionKeypointsDataset(df)\n",
    "print(f'Dataset size: {len(full_dataset)}')\n",
    "\n",
    "# Split train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for keypoints, labels in loader:\n",
    "        keypoints = keypoints.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for keypoints, labels in loader:\n",
    "            keypoints = keypoints.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(keypoints)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja\n",
    "model = KeypointsEmotionMLP(\n",
    "    input_dim=INPUT_FEATURES,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    num_classes=NUM_EMOTIONS,\n",
    "    dropout=DROPOUT,\n",
    ").to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n",
    ")\n",
    "\n",
    "# Historia\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': [],\n",
    "}\n",
    "best_val_acc = 0\n",
    "best_model_state = None\n",
    "\n",
    "print(f'Starting training for {EPOCHS} epochs...')\n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save(best_model_state, 'emotion_keypoints.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch+1:3d}/{EPOCHS} | '\n",
    "            f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | '\n",
    "            f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% | '\n",
    "            f'Best: {best_val_acc:.2f}%'\n",
    "        )\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'Training complete! Best validation accuracy: {best_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Wizualizacja wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ewaluacja końcowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Załaduj najlepszy model\n",
    "model.load_state_dict(torch.load('emotion_keypoints.pt'))\n",
    "model.eval()\n",
    "\n",
    "# Predykcje na validation set\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for keypoints, labels in val_loader:\n",
    "        keypoints = keypoints.to(device)\n",
    "        outputs = model(keypoints)\n",
    "        _, predicted = outputs.max(1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Classification report\n",
    "print('Classification Report:')\n",
    "print(classification_report(\n",
    "    all_labels, all_preds,\n",
    "    target_names=EMOTION_CLASSES,\n",
    "    digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    cm, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=EMOTION_CLASSES,\n",
    "    yticklabels=EMOTION_CLASSES,\n",
    ")\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zapisz model i metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz finalny model\n",
    "torch.save(best_model_state, 'emotion_keypoints.pt')\n",
    "print('Model saved: emotion_keypoints.pt')\n",
    "\n",
    "# Zapisz metryki\n",
    "metrics = {\n",
    "    'best_val_accuracy': best_val_acc,\n",
    "    'epochs': EPOCHS,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'learning_rate': LEARNING_RATE,\n",
    "    'hidden_dims': HIDDEN_DIMS,\n",
    "    'dropout': DROPOUT,\n",
    "    'num_keypoints': NUM_KEYPOINTS,\n",
    "    'input_features': INPUT_FEATURES,\n",
    "    'emotion_classes': EMOTION_CLASSES,\n",
    "    'train_samples': len(train_dataset),\n",
    "    'val_samples': len(val_dataset),\n",
    "    'history': history,\n",
    "}\n",
    "\n",
    "with open('emotion_keypoints_metrics.json', 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print('Metrics saved: emotion_keypoints_metrics.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Weryfikacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test że model można załadować i użyć\n",
    "test_model = KeypointsEmotionMLP(\n",
    "    input_dim=INPUT_FEATURES,\n",
    "    hidden_dims=HIDDEN_DIMS,\n",
    "    num_classes=NUM_EMOTIONS,\n",
    "    dropout=DROPOUT,\n",
    ")\n",
    "test_model.load_state_dict(torch.load('emotion_keypoints.pt'))\n",
    "test_model.eval()\n",
    "\n",
    "# Test inference\n",
    "dummy_input = torch.randn(1, INPUT_FEATURES)\n",
    "with torch.no_grad():\n",
    "    output = test_model(dummy_input)\n",
    "    probs = torch.softmax(output, dim=1)[0]\n",
    "\n",
    "print('Test inference:')\n",
    "print(f'Input shape: {dummy_input.shape}')\n",
    "print(f'Output shape: {output.shape}')\n",
    "print('Probabilities:')\n",
    "for i, emotion in enumerate(EMOTION_CLASSES):\n",
    "    print(f'  {emotion}: {probs[i].item():.2%}')\n",
    "\n",
    "print('\\n✓ Model verified successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Instrukcje po treningu\n",
    "\n",
    "### Pobierz następujące pliki:\n",
    "\n",
    "1. **`emotion_keypoints.pt`** - wagi modelu (WYMAGANE)\n",
    "2. **`emotion_keypoints_metrics.json`** - metryki treningu\n",
    "3. **`training_history.png`** - wykres treningu\n",
    "4. **`confusion_matrix.png`** - macierz pomyłek\n",
    "\n",
    "### Przekaż plik `emotion_keypoints.pt` do projektu:\n",
    "\n",
    "Umieść go w katalogu `models/` projektu.\n",
    "\n",
    "### Oczekiwana dokładność:\n",
    "\n",
    "- Na syntetycznych danych: ~16-20% (random baseline dla 6 klas)\n",
    "- Na prawdziwych danych: 40-70% (zależnie od jakości)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
