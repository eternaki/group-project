{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Dog Emotion Classification - Training with Action Units (DogFACS)\n\nTrening modelu klasyfikacji emocji psów zgodnie z **DogFACS** (Dog Facial Action Coding System).\n\n**Architektura:**\n```\nKeypoints (60) → Action Units (12) → łącznie 72 features → MLP → 5 klas\n```\n\n**Action Units (AU) obliczane z geometrii keypoints:**\n- AU_brow_raise - podniesienie brwi\n- AU_ear_forward - uszy do przodu (pozytywna emocja)\n- AU_ear_back - uszy do tyłu (negatywna emocja)\n- AU_mouth_open - otwarcie pyska\n- AU_lip_corner_pull - \"uśmiech\"\n- i inne...\n\n**Wymagane datasety Kaggle:**\n1. `lovodkin/dogflw` - keypoints twarzy psów\n2. `dougandrade/dog-emotions-5-classes` - obrazy z etykietami emocji\n\n**Po zakończeniu:**\nPobierz `emotion_keypoints.pt` i przekaż do projektu."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalacja zależności\n",
    "!pip install -q timm albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === KONFIGURACJA ===\n\n# Ścieżki do datasetów (dostosuj do Kaggle)\nDOGFLW_PATH = '/kaggle/input/dogflw'  # Dataset z keypoints\nEMOTIONS_PATH = '/kaggle/input/dog-emotions-5-classes'  # Dataset z emocjami\n\n# Parametry modelu\nNUM_KEYPOINTS = 20\nKEYPOINTS_FEATURES = NUM_KEYPOINTS * 3  # 60\n\n# Action Units (obliczane z keypoints zgodnie z DogFACS)\nNUM_ACTION_UNITS = 12\nAU_FEATURES = NUM_ACTION_UNITS  # 12\n\n# Łączna liczba cech wejściowych\nINPUT_FEATURES = KEYPOINTS_FEATURES + AU_FEATURES  # 60 + 12 = 72\n\n# 5 klas trenowanych (neutral wykrywany przez próg confidence)\nNUM_EMOTIONS_TRAINED = 5\nEMOTION_CLASSES_TRAINED = ['happy', 'sad', 'angry', 'fearful', 'relaxed']\n\n# Wszystkie klasy (6) - neutral dodawany automatycznie w inference\nEMOTION_CLASSES_ALL = EMOTION_CLASSES_TRAINED + ['neutral']\n\n# Mapping emocji z datasetu do naszych klas\nEMOTION_MAPPING = {\n    'happy': 0,\n    'sad': 1,\n    'angry': 2,\n    'fear': 3,\n    'fearful': 3,\n    'relaxed': 4,\n}\n\n# Parametry treningu\nBATCH_SIZE = 64\nEPOCHS = 100\nLEARNING_RATE = 0.001\nHIDDEN_DIMS = [256, 128, 64]\nDROPOUT = 0.3\n\n# Próg dla wykrywania neutral\nNEUTRAL_THRESHOLD = 0.35\n\nprint(f'Keypoints features: {KEYPOINTS_FEATURES}')\nprint(f'Action Units features: {AU_FEATURES}')\nprint(f'Total input features: {INPUT_FEATURES}')\nprint(f'Training classes ({NUM_EMOTIONS_TRAINED}): {EMOTION_CLASSES_TRAINED}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Mapping DogFLW (46) → Project (20) Keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwy 20 keypoints projektu\n",
    "KEYPOINT_NAMES = [\n",
    "    \"left_eye\",           # 0\n",
    "    \"right_eye\",          # 1\n",
    "    \"nose\",               # 2\n",
    "    \"left_ear_base\",      # 3\n",
    "    \"right_ear_base\",     # 4\n",
    "    \"left_ear_tip\",       # 5\n",
    "    \"right_ear_tip\",      # 6\n",
    "    \"left_mouth_corner\",  # 7\n",
    "    \"right_mouth_corner\", # 8\n",
    "    \"upper_lip\",          # 9\n",
    "    \"lower_lip\",          # 10\n",
    "    \"chin\",               # 11\n",
    "    \"left_cheek\",         # 12\n",
    "    \"right_cheek\",        # 13\n",
    "    \"forehead\",           # 14\n",
    "    \"left_eyebrow\",       # 15\n",
    "    \"right_eyebrow\",      # 16\n",
    "    \"muzzle_top\",         # 17\n",
    "    \"muzzle_left\",        # 18\n",
    "    \"muzzle_right\",       # 19\n",
    "]\n",
    "\n",
    "# Mapping: DogFLW index → Project index\n",
    "DOGFLW_TO_PROJECT = {\n",
    "    0: 0,   # left_eye\n",
    "    1: 1,   # right_eye\n",
    "    14: 2,  # nose\n",
    "    32: 3,  # left_ear_base\n",
    "    36: 4,  # right_ear_base\n",
    "    34: 5,  # left_ear_tip\n",
    "    38: 6,  # right_ear_tip\n",
    "    20: 7,  # left_mouth_corner\n",
    "    24: 8,  # right_mouth_corner\n",
    "    22: 9,  # upper_lip\n",
    "    26: 10, # lower_lip\n",
    "    28: 11, # chin\n",
    "    4: 12,  # left_cheek\n",
    "    8: 13,  # right_cheek\n",
    "    40: 14, # forehead\n",
    "    42: 15, # left_eyebrow\n",
    "    44: 16, # right_eyebrow\n",
    "    16: 17, # muzzle_top\n",
    "    6: 18,  # muzzle_left\n",
    "    10: 19, # muzzle_right\n",
    "}\n",
    "\n",
    "# Odwrotny mapping: Project index → DogFLW index\n",
    "PROJECT_TO_DOGFLW = {v: k for k, v in DOGFLW_TO_PROJECT.items()}\n",
    "\n",
    "print(f'Mapping zdefiniowany: {len(PROJECT_TO_DOGFLW)} keypoints')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Keypoints (do ekstrakcji z obrazów)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from torchvision import transforms\n",
    "\n",
    "class SimpleBaselineModel(nn.Module):\n",
    "    \"\"\"Model do detekcji 46 keypoints DogFLW.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_keypoints=46):\n",
    "        super().__init__()\n",
    "        self.bb = timm.create_model(\n",
    "            'resnet50',\n",
    "            pretrained=False,\n",
    "            features_only=True,\n",
    "            out_indices=[-1],\n",
    "        )\n",
    "        self.head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2048, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 256, 4, 2, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.out = nn.Conv2d(256, num_keypoints, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.bb(x)[-1]\n",
    "        x = self.head(x)\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "class KeypointsExtractor:\n",
    "    \"\"\"Ekstraktor keypoints z obrazów.\"\"\"\n",
    "    \n",
    "    def __init__(self, weights_path=None):\n",
    "        self.model = SimpleBaselineModel(num_keypoints=46)\n",
    "        self.device = device\n",
    "        \n",
    "        if weights_path and os.path.exists(weights_path):\n",
    "            state_dict = torch.load(weights_path, map_location=device)\n",
    "            self.model.load_state_dict(state_dict)\n",
    "            print(f'Loaded keypoints weights: {weights_path}')\n",
    "        else:\n",
    "            print('WARNING: No keypoints weights loaded!')\n",
    "        \n",
    "        self.model = self.model.to(device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "            ),\n",
    "        ])\n",
    "    \n",
    "    def extract(self, image):\n",
    "        \"\"\"Ekstrahuje 20 keypoints z obrazu.\"\"\"\n",
    "        if isinstance(image, str):\n",
    "            image = cv2.imread(image)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        h, w = image.shape[:2]\n",
    "        tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            heatmaps = self.model(tensor)[0]  # (46, H, W)\n",
    "        \n",
    "        # Dekoduj 46 keypoints\n",
    "        dogflw_keypoints = self._decode_heatmaps(heatmaps, w, h)\n",
    "        \n",
    "        # Mapuj do 20 keypoints\n",
    "        project_keypoints = self._map_to_project(dogflw_keypoints)\n",
    "        \n",
    "        return project_keypoints\n",
    "    \n",
    "    def _decode_heatmaps(self, heatmaps, target_w, target_h):\n",
    "        \"\"\"Dekoduje heatmapy do współrzędnych.\"\"\"\n",
    "        hm_h, hm_w = heatmaps.shape[1], heatmaps.shape[2]\n",
    "        scale_x = target_w / hm_w\n",
    "        scale_y = target_h / hm_h\n",
    "        \n",
    "        keypoints = []\n",
    "        for k in range(46):\n",
    "            hm = heatmaps[k].cpu().numpy()\n",
    "            max_val = hm.max()\n",
    "            max_idx = hm.argmax()\n",
    "            y_hm = max_idx // hm_w\n",
    "            x_hm = max_idx % hm_w\n",
    "            \n",
    "            x = float(x_hm * scale_x)\n",
    "            y = float(y_hm * scale_y)\n",
    "            visibility = float(max_val)\n",
    "            \n",
    "            keypoints.append((x, y, visibility))\n",
    "        \n",
    "        return keypoints\n",
    "    \n",
    "    def _map_to_project(self, dogflw_keypoints):\n",
    "        \"\"\"Mapuje 46 → 20 keypoints.\"\"\"\n",
    "        project_keypoints = []\n",
    "        for proj_idx in range(20):\n",
    "            dogflw_idx = PROJECT_TO_DOGFLW[proj_idx]\n",
    "            project_keypoints.append(dogflw_keypoints[dogflw_idx])\n",
    "        return project_keypoints\n",
    "\n",
    "\n",
    "print('KeypointsExtractor defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Action Units (DogFACS)\n\nObliczamy 12 Action Units na podstawie geometrii keypoints zgodnie z DogFACS."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import math\n\n# Indeksy keypoints\nKP_LEFT_EYE, KP_RIGHT_EYE = 0, 1\nKP_NOSE = 2\nKP_LEFT_EAR_BASE, KP_RIGHT_EAR_BASE = 3, 4\nKP_LEFT_EAR_TIP, KP_RIGHT_EAR_TIP = 5, 6\nKP_LEFT_MOUTH, KP_RIGHT_MOUTH = 7, 8\nKP_UPPER_LIP, KP_LOWER_LIP = 9, 10\nKP_CHIN = 11\nKP_LEFT_CHEEK, KP_RIGHT_CHEEK = 12, 13\nKP_FOREHEAD = 14\nKP_LEFT_BROW, KP_RIGHT_BROW = 15, 16\nKP_MUZZLE_TOP, KP_MUZZLE_LEFT, KP_MUZZLE_RIGHT = 17, 18, 19\n\n# Nazwy Action Units\nACTION_UNIT_NAMES = [\n    \"AU_brow_raise\",      # Podniesienie brwi\n    \"AU_ear_forward\",     # Uszy do przodu (pozytywna emocja)\n    \"AU_ear_back\",        # Uszy do tyłu (negatywna emocja)\n    \"AU_ear_asymmetry\",   # Asymetria uszu\n    \"AU_eye_opening\",     # Otwarcie oczu\n    \"AU_mouth_open\",      # Otwarcie pyska\n    \"AU_lip_corner_pull\", # Pociągnięcie kącików ust (uśmiech)\n    \"AU_jaw_drop\",        # Opadnięcie szczęki\n    \"AU_nose_wrinkle\",    # Zmarszczenie nosa\n    \"AU_muzzle_width\",    # Szerokość pyska\n    \"AU_face_elongation\", # Wydłużenie twarzy\n    \"AU_eye_distance\",    # Odległość między oczami\n]\n\ndef extract_action_units(keypoints_flat):\n    \"\"\"\n    Ekstrahuje 12 Action Units z keypoints.\n    \n    Args:\n        keypoints_flat: Array [x0,y0,v0, x1,y1,v1, ...] (60 wartości)\n    \n    Returns:\n        Array z 12 wartościami AU w zakresie [0, 1]\n    \"\"\"\n    kp = keypoints_flat.reshape(20, 3)\n    coords = kp[:, :2]\n    visibility = kp[:, 2]\n    \n    def distance(p1, p2):\n        return float(np.sqrt(np.sum((p1 - p2) ** 2)))\n    \n    def angle(p1, p2):\n        dx, dy = p2[0] - p1[0], p2[1] - p1[1]\n        return math.atan2(dy, dx)\n    \n    # Odległość referencyjna\n    eye_dist = max(distance(coords[KP_LEFT_EYE], coords[KP_RIGHT_EYE]), 1e-6)\n    \n    # AU_brow_raise\n    brow_dist = (distance(coords[KP_LEFT_BROW], coords[KP_LEFT_EYE]) +\n                 distance(coords[KP_RIGHT_BROW], coords[KP_RIGHT_EYE])) / 2\n    au_brow = np.clip((brow_dist / eye_dist - 0.2) / 0.3, 0, 1)\n    \n    # AU_ear_forward\n    left_ang = angle(coords[KP_LEFT_EAR_BASE], coords[KP_LEFT_EAR_TIP])\n    right_ang = angle(coords[KP_RIGHT_EAR_BASE], coords[KP_RIGHT_EAR_TIP])\n    avg_ang = (abs(left_ang) + abs(right_ang)) / 2\n    au_ear_fwd = np.clip(1.0 - avg_ang / math.pi, 0, 1)\n    \n    # AU_ear_back\n    au_ear_back = 1.0 - au_ear_fwd\n    \n    # AU_ear_asymmetry\n    au_ear_asym = np.clip(abs(left_ang - right_ang) / math.pi, 0, 1)\n    \n    # AU_eye_opening\n    au_eye = np.clip((visibility[KP_LEFT_EYE] + visibility[KP_RIGHT_EYE]) / 2, 0, 1)\n    \n    # AU_mouth_open\n    lip_dist = distance(coords[KP_UPPER_LIP], coords[KP_LOWER_LIP])\n    au_mouth = np.clip(lip_dist / eye_dist / 0.3, 0, 1)\n    \n    # AU_lip_corner_pull\n    left_lip_ang = angle(coords[KP_UPPER_LIP], coords[KP_LEFT_MOUTH])\n    right_lip_ang = angle(coords[KP_UPPER_LIP], coords[KP_RIGHT_MOUTH])\n    au_smile = np.clip((left_lip_ang - right_lip_ang + math.pi) / (2 * math.pi), 0, 1)\n    \n    # AU_jaw_drop\n    nose_chin = distance(coords[KP_NOSE], coords[KP_CHIN])\n    au_jaw = np.clip((nose_chin / eye_dist - 0.5) / 1.0, 0, 1)\n    \n    # AU_nose_wrinkle\n    nose_lip = distance(coords[KP_NOSE], coords[KP_UPPER_LIP])\n    au_nose = np.clip(1.0 - nose_lip / eye_dist / 0.5, 0, 1)\n    \n    # AU_muzzle_width\n    muzzle_w = distance(coords[KP_MUZZLE_LEFT], coords[KP_MUZZLE_RIGHT])\n    au_muzzle = np.clip((muzzle_w / eye_dist - 0.3) / 0.5, 0, 1)\n    \n    # AU_face_elongation\n    face_h = distance(coords[KP_FOREHEAD], coords[KP_CHIN])\n    au_elong = np.clip((face_h / eye_dist - 1.0) / 1.5, 0, 1)\n    \n    # AU_eye_distance\n    au_eye_dist = np.clip(eye_dist / 100.0, 0, 1)\n    \n    return np.array([\n        au_brow, au_ear_fwd, au_ear_back, au_ear_asym,\n        au_eye, au_mouth, au_smile, au_jaw,\n        au_nose, au_muzzle, au_elong, au_eye_dist\n    ], dtype=np.float32)\n\n# Test\ntest_kp = np.random.rand(60).astype(np.float32) * 100\nfor i in range(20):\n    test_kp[i*3 + 2] = np.random.rand()  # visibility [0,1]\nau = extract_action_units(test_kp)\nprint(f'Action Units shape: {au.shape}')\nprint(f'AU values: {dict(zip(ACTION_UNIT_NAMES, au))}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Przygotowanie danych\n",
    "\n",
    "**Opcja A:** Użyj gotowych keypoints z DogFLW + etykiet emocji\n",
    "\n",
    "**Opcja B:** Ekstrahuj keypoints z obrazów emotion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź dostępne datasety\n",
    "print('=== Dostępne datasety ===')\n",
    "\n",
    "if os.path.exists(DOGFLW_PATH):\n",
    "    print(f'✓ DogFLW: {DOGFLW_PATH}')\n",
    "    print(f'  Files: {os.listdir(DOGFLW_PATH)[:5]}...')\n",
    "else:\n",
    "    print(f'✗ DogFLW not found at {DOGFLW_PATH}')\n",
    "\n",
    "if os.path.exists(EMOTIONS_PATH):\n",
    "    print(f'✓ Emotions: {EMOTIONS_PATH}')\n",
    "    print(f'  Files: {os.listdir(EMOTIONS_PATH)[:5]}...')\n",
    "else:\n",
    "    print(f'✗ Emotions not found at {EMOTIONS_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPCJA A: Załaduj DogFLW keypoints ===\n",
    "# DogFLW zawiera keypoints, ale potrzebujemy etykiet emocji\n",
    "\n",
    "def load_dogflw_keypoints(dogflw_path):\n",
    "    \"\"\"Ładuje keypoints z DogFLW dataset.\"\"\"\n",
    "    # Szukaj pliku z anotacjami\n",
    "    for filename in ['landmarks.csv', 'annotations.csv', 'keypoints.csv']:\n",
    "        filepath = os.path.join(dogflw_path, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f'Loaded: {filepath}')\n",
    "            print(f'Columns: {list(df.columns)[:10]}...')\n",
    "            return df\n",
    "    \n",
    "    print('No keypoints CSV found in DogFLW')\n",
    "    return None\n",
    "\n",
    "# dogflw_df = load_dogflw_keypoints(DOGFLW_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPCJA B: Ekstrahuj keypoints z emotion dataset ===\n",
    "\n",
    "def prepare_emotion_dataset(emotions_path, keypoints_extractor=None):\n",
    "    \"\"\"\n",
    "    Przygotowuje dataset z emotion images.\n",
    "    \n",
    "    Struktura oczekiwana:\n",
    "    emotions_path/\n",
    "        happy/\n",
    "            img1.jpg\n",
    "            img2.jpg\n",
    "        sad/\n",
    "            ...\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for emotion_folder in os.listdir(emotions_path):\n",
    "        folder_path = os.path.join(emotions_path, emotion_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        \n",
    "        emotion_name = emotion_folder.lower()\n",
    "        if emotion_name not in EMOTION_MAPPING:\n",
    "            print(f'Skipping unknown emotion: {emotion_name}')\n",
    "            continue\n",
    "        \n",
    "        emotion_id = EMOTION_MAPPING[emotion_name]\n",
    "        \n",
    "        images = [f for f in os.listdir(folder_path) \n",
    "                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        \n",
    "        print(f'{emotion_name}: {len(images)} images')\n",
    "        \n",
    "        for img_name in tqdm(images, desc=emotion_name):\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            \n",
    "            if keypoints_extractor:\n",
    "                try:\n",
    "                    keypoints = keypoints_extractor.extract(img_path)\n",
    "                    # Flatten: [(x0,y0,v0), (x1,y1,v1), ...] → [x0,y0,v0,x1,y1,v1,...]\n",
    "                    flat_keypoints = []\n",
    "                    for x, y, v in keypoints:\n",
    "                        flat_keypoints.extend([x, y, v])\n",
    "                    \n",
    "                    data.append({\n",
    "                        'image_path': img_path,\n",
    "                        'emotion_id': emotion_id,\n",
    "                        'emotion_name': emotion_name,\n",
    "                        'keypoints': flat_keypoints,\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                    print(f'Error processing {img_path}: {e}')\n",
    "            else:\n",
    "                data.append({\n",
    "                    'image_path': img_path,\n",
    "                    'emotion_id': emotion_id,\n",
    "                    'emotion_name': emotion_name,\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "print('prepare_emotion_dataset() defined')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Załaduj/Przygotuj dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj model keypoints (jeśli masz wagi)\n",
    "KEYPOINTS_WEIGHTS = '/kaggle/input/dogflw/keypoints_best.pt'  # Dostosuj ścieżkę\n",
    "\n",
    "if os.path.exists(KEYPOINTS_WEIGHTS):\n",
    "    extractor = KeypointsExtractor(KEYPOINTS_WEIGHTS)\n",
    "else:\n",
    "    print('Keypoints weights not found. Using None (will generate synthetic data)')\n",
    "    extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Przygotuj dataset\nif extractor and os.path.exists(EMOTIONS_PATH):\n    print('Extracting keypoints from emotion images...')\n    df = prepare_emotion_dataset(EMOTIONS_PATH, extractor)\n    \n    # Zapisz do CSV dla przyszłego użycia\n    df.to_csv('emotion_keypoints_dataset.csv', index=False)\n    print(f'Dataset saved: emotion_keypoints_dataset.csv ({len(df)} samples)')\nelse:\n    print('Using synthetic data for testing...')\n    # Generuj syntetyczne dane dla 5 klas\n    n_samples = 5000\n    np.random.seed(42)\n    \n    synthetic_keypoints = np.random.randn(n_samples, INPUT_FEATURES).astype(np.float32)\n    synthetic_labels = np.random.randint(0, NUM_EMOTIONS_TRAINED, n_samples)  # 5 klas\n    \n    df = pd.DataFrame({\n        'emotion_id': synthetic_labels,\n        'keypoints': [list(kp) for kp in synthetic_keypoints],\n    })\n    print(f'Synthetic dataset: {len(df)} samples')\n\nprint(f'\\nDataset shape: {df.shape}')\nprint(f'Emotion distribution:')\nfor i, name in enumerate(EMOTION_CLASSES_TRAINED):\n    count = (df['emotion_id'] == i).sum()\n    print(f'  {i}: {name} - {count} samples')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset i DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionKeypointsDataset(Dataset):\n",
    "    \"\"\"Dataset dla treningu emotion classifier.\"\"\"\n",
    "    \n",
    "    def __init__(self, dataframe):\n",
    "        self.df = dataframe.reset_index(drop=True)\n",
    "        \n",
    "        # Przygotuj keypoints jako numpy array\n",
    "        if isinstance(self.df['keypoints'].iloc[0], str):\n",
    "            # Jeśli keypoints są zapisane jako string\n",
    "            self.keypoints = np.array([\n",
    "                eval(kp) for kp in self.df['keypoints']\n",
    "            ], dtype=np.float32)\n",
    "        else:\n",
    "            self.keypoints = np.array(\n",
    "                self.df['keypoints'].tolist(), dtype=np.float32\n",
    "            )\n",
    "        \n",
    "        self.labels = self.df['emotion_id'].values.astype(np.int64)\n",
    "        \n",
    "        # Normalizacja keypoints\n",
    "        self.keypoints = self._normalize(self.keypoints)\n",
    "    \n",
    "    def _normalize(self, keypoints):\n",
    "        \"\"\"Normalizuje keypoints.\"\"\"\n",
    "        # Dla każdej próbki, normalizuj x,y względem zakresu\n",
    "        normalized = keypoints.copy()\n",
    "        \n",
    "        for i in range(len(normalized)):\n",
    "            kp = normalized[i]\n",
    "            # x: indices 0, 3, 6, ...\n",
    "            # y: indices 1, 4, 7, ...\n",
    "            # v: indices 2, 5, 8, ...\n",
    "            xs = kp[0::3]\n",
    "            ys = kp[1::3]\n",
    "            \n",
    "            # Normalizuj do [-1, 1]\n",
    "            if xs.max() > xs.min():\n",
    "                xs = 2 * (xs - xs.min()) / (xs.max() - xs.min()) - 1\n",
    "            if ys.max() > ys.min():\n",
    "                ys = 2 * (ys - ys.min()) / (ys.max() - ys.min()) - 1\n",
    "            \n",
    "            kp[0::3] = xs\n",
    "            kp[1::3] = ys\n",
    "            # visibility pozostaje bez zmian\n",
    "        \n",
    "        return normalized\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.from_numpy(self.keypoints[idx]),\n",
    "            torch.tensor(self.labels[idx]),\n",
    "        )\n",
    "\n",
    "\n",
    "# Stwórz dataset\n",
    "full_dataset = EmotionKeypointsDataset(df)\n",
    "print(f'Dataset size: {len(full_dataset)}')\n",
    "\n",
    "# Split train/val\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(\n",
    "    full_dataset, [train_size, val_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Val: {len(val_dataset)}')\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for keypoints, labels in loader:\n",
    "        keypoints = keypoints.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(keypoints)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for keypoints, labels in loader:\n",
    "            keypoints = keypoints.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(keypoints)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    return total_loss / len(loader), 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Inicjalizacja\nmodel = KeypointsEmotionMLP(\n    input_dim=INPUT_FEATURES,\n    hidden_dims=HIDDEN_DIMS,\n    num_classes=NUM_EMOTIONS_TRAINED,  # 5 klas\n    dropout=DROPOUT,\n).to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=10, verbose=True\n)\n\n# Historia\nhistory = {\n    'train_loss': [], 'train_acc': [],\n    'val_loss': [], 'val_acc': [],\n}\nbest_val_acc = 0\nbest_model_state = None\n\nprint(f'Starting training for {EPOCHS} epochs...')\nprint(f'Classes: {NUM_EMOTIONS_TRAINED} (neutral detected by threshold < {NEUTRAL_THRESHOLD})')\nprint('=' * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trening\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_model_state = model.state_dict().copy()\n",
    "        torch.save(best_model_state, 'emotion_keypoints.pt')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(\n",
    "            f'Epoch {epoch+1:3d}/{EPOCHS} | '\n",
    "            f'Train Loss: {train_loss:.4f} Acc: {train_acc:.2f}% | '\n",
    "            f'Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}% | '\n",
    "            f'Best: {best_val_acc:.2f}%'\n",
    "        )\n",
    "\n",
    "print('=' * 70)\n",
    "print(f'Training complete! Best validation accuracy: {best_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Wizualizacja wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history['train_loss'], label='Train Loss')\n",
    "axes[0].plot(history['val_loss'], label='Val Loss')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Accuracy\n",
    "axes[1].plot(history['train_acc'], label='Train Acc')\n",
    "axes[1].plot(history['val_acc'], label='Val Acc')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_history.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ewaluacja końcowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Załaduj najlepszy model\nmodel.load_state_dict(torch.load('emotion_keypoints.pt'))\nmodel.eval()\n\n# Predykcje na validation set\nall_preds = []\nall_labels = []\n\nwith torch.no_grad():\n    for keypoints, labels in val_loader:\n        keypoints = keypoints.to(device)\n        outputs = model(keypoints)\n        _, predicted = outputs.max(1)\n        \n        all_preds.extend(predicted.cpu().numpy())\n        all_labels.extend(labels.numpy())\n\n# Classification report (5 klas trenowanych)\nprint('Classification Report (5 trained classes):')\nprint(classification_report(\n    all_labels, all_preds,\n    target_names=EMOTION_CLASSES_TRAINED,\n    digits=3\n))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix (5 klas trenowanych)\ncm = confusion_matrix(all_labels, all_preds)\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    cm, annot=True, fmt='d', cmap='Blues',\n    xticklabels=EMOTION_CLASSES_TRAINED,\n    yticklabels=EMOTION_CLASSES_TRAINED,\n)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix (5 trained classes)\\nNeutral is detected by low confidence in inference')\nplt.tight_layout()\nplt.savefig('confusion_matrix.png', dpi=150)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Zapisz model i metryki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Zapisz finalny model\ntorch.save(best_model_state, 'emotion_keypoints.pt')\nprint('Model saved: emotion_keypoints.pt')\n\n# Zapisz metryki\nmetrics = {\n    'best_val_accuracy': best_val_acc,\n    'epochs': EPOCHS,\n    'batch_size': BATCH_SIZE,\n    'learning_rate': LEARNING_RATE,\n    'hidden_dims': HIDDEN_DIMS,\n    'dropout': DROPOUT,\n    'num_keypoints': NUM_KEYPOINTS,\n    'input_features': INPUT_FEATURES,\n    'num_classes_trained': NUM_EMOTIONS_TRAINED,\n    'emotion_classes_trained': EMOTION_CLASSES_TRAINED,\n    'emotion_classes_all': EMOTION_CLASSES_ALL,\n    'neutral_threshold': NEUTRAL_THRESHOLD,\n    'train_samples': len(train_dataset),\n    'val_samples': len(val_dataset),\n    'history': history,\n}\n\nwith open('emotion_keypoints_metrics.json', 'w') as f:\n    json.dump(metrics, f, indent=2)\n\nprint('Metrics saved: emotion_keypoints_metrics.json')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Weryfikacja modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Test że model można załadować i użyć\ntest_model = KeypointsEmotionMLP(\n    input_dim=INPUT_FEATURES,\n    hidden_dims=HIDDEN_DIMS,\n    num_classes=NUM_EMOTIONS_TRAINED,  # 5 klas\n    dropout=DROPOUT,\n)\ntest_model.load_state_dict(torch.load('emotion_keypoints.pt'))\ntest_model.eval()\n\n# Test inference\ndummy_input = torch.randn(1, INPUT_FEATURES)\nwith torch.no_grad():\n    output = test_model(dummy_input)\n    probs = torch.softmax(output, dim=1)[0]\n\nprint('Test inference:')\nprint(f'Input shape: {dummy_input.shape}')\nprint(f'Output shape: {output.shape} (5 trained classes)')\nprint('\\nProbabilities (trained classes):')\nfor i, emotion in enumerate(EMOTION_CLASSES_TRAINED):\n    print(f'  {emotion}: {probs[i].item():.2%}')\n\n# Symulacja detekcji neutral\nmax_prob = probs.max().item()\nprint(f'\\nMax probability: {max_prob:.2%}')\nif max_prob < NEUTRAL_THRESHOLD:\n    print(f'→ Would be classified as NEUTRAL (below threshold {NEUTRAL_THRESHOLD})')\nelse:\n    print(f'→ Would be classified as {EMOTION_CLASSES_TRAINED[probs.argmax()]}')\n\nprint('\\n✓ Model verified successfully!')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Instrukcje po treningu\n\n### Pobierz następujące pliki:\n\n1. **`emotion_keypoints.pt`** - wagi modelu (WYMAGANE)\n2. **`emotion_keypoints_metrics.json`** - metryki treningu\n3. **`training_history.png`** - wykres treningu\n4. **`confusion_matrix.png`** - macierz pomyłek\n\n### Przekaż plik `emotion_keypoints.pt` do projektu:\n\nUmieść go w katalogu `models/` projektu.\n\n### Jak działa model:\n\n```\nModel trenowany na 5 klasach:\n  happy, sad, angry, fearful, relaxed\n\nW inference (EmotionModel.predict):\n  if max(probability) < 0.35:\n      return \"neutral\"\n  else:\n      return klasa z max(probability)\n```\n\n### Oczekiwana dokładność:\n\n- Na syntetycznych danych: ~20% (random baseline dla 5 klas)\n- Na prawdziwych danych: **40-70%** (zależnie od jakości)\n\n---"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}