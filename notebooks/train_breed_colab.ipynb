{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêï –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ –ø–æ—Ä–æ–¥ —Å–æ–±–∞–∫\n",
    "\n",
    "**Sprint 3: Breed Classification**\n",
    "\n",
    "–≠—Ç–æ—Ç notebook –æ–±—É—á–∞–µ—Ç EfficientNet-B4 –Ω–∞ Stanford Dogs Dataset.\n",
    "\n",
    "## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è\n",
    "- Google Colab —Å GPU (Runtime ‚Üí Change runtime type ‚Üí GPU)\n",
    "- Google Drive –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–µ—Å–æ–≤\n",
    "\n",
    "## –†–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "- –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å `breed_best.pt`\n",
    "- Top-5 accuracy > 80%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. –ü—Ä–æ–≤–µ—Ä–∫–∞ GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport os\n\n# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∞–ø–∫—É –ø—Ä–æ–µ–∫—Ç–∞ (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø–æ–∑–∂–µ)\nPROJECT_DIR = '/content/drive/MyDrive/DogFACS'\n\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    print(f\"‚úÖ GPU –¥–æ—Å—Ç—É–ø–µ–Ω: {gpu_name}\")\n    print(f\"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\nelse:\n    print(\"‚ùå GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω!\")\n    print(\"   –ü–µ—Ä–µ–π–¥–∏ –≤ Runtime ‚Üí Change runtime type ‚Üí GPU\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q timm kaggle tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ Google Drive\n",
    "\n",
    "–î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# –°–æ–∑–¥–∞—ë–º –ø–∞–ø–∫—É –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞\n",
    "import os\n",
    "PROJECT_DIR = '/content/drive/MyDrive/DogFACS'\n",
    "os.makedirs(PROJECT_DIR, exist_ok=True)\n",
    "print(f\"‚úÖ –ü–∞–ø–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞: {PROJECT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 3. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ Stanford Dogs Dataset\n\n### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kaggle API\n\n**–í–∞—Ä–∏–∞–Ω—Ç A (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è):** –í–≤–µ–¥–∏ –¥–∞–Ω–Ω—ã–µ –ø—Ä—è–º–æ –≤ –∫–æ–¥ –Ω–∏–∂–µ:\n1. –ó–∞–π–¥–∏ –Ω–∞ https://www.kaggle.com/settings\n2. –ù–∞–π–¥–∏ —Å–µ–∫—Ü–∏—é \"API\" –∏ —Å–∫–æ–ø–∏—Ä—É–π:\n   - Username (—Ç–≤–æ–π –ª–æ–≥–∏–Ω –Ω–∞ Kaggle)\n   - API Key (–Ω–∞–∂–º–∏ \"Create New Token\" –µ—Å–ª–∏ –Ω–µ—Ç)\n3. –í—Å—Ç–∞–≤—å –≤ —è—á–µ–π–∫—É –Ω–∏–∂–µ\n\n**–í–∞—Ä–∏–∞–Ω—Ç B:** –ó–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª `kaggle.json` –µ—Å–ª–∏ –æ–Ω —É —Ç–µ–±—è –µ—Å—Ç—å"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Kaggle API\n# –í–∞—Ä–∏–∞–Ω—Ç 1: –í–≤–µ–¥–∏ username –∏ API key –≤—Ä—É—á–Ω—É—é\nKAGGLE_USERNAME = \"\"  # ‚Üê –í–≤–µ–¥–∏ —Å–≤–æ–π username\nKAGGLE_KEY = \"\"       # ‚Üê –í–≤–µ–¥–∏ —Å–≤–æ–π API key\n\n# –ï—Å–ª–∏ –∑–∞–ø–æ–ª–Ω–∏–ª –≤—ã—à–µ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö\nif KAGGLE_USERNAME and KAGGLE_KEY:\n    import json\n    kaggle_config = {\"username\": KAGGLE_USERNAME, \"key\": KAGGLE_KEY}\n    \n    !mkdir -p ~/.kaggle\n    with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n        json.dump(kaggle_config, f)\n    !chmod 600 ~/.kaggle/kaggle.json\n    print(\"‚úÖ Kaggle –Ω–∞—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ –≤–≤–æ–¥ –∫–ª—é—á–∞\")\n\n# –í–∞—Ä–∏–∞–Ω—Ç 2: –ó–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª kaggle.json (–µ—Å–ª–∏ –µ—Å—Ç—å)\nelse:\n    from google.colab import files\n    print(\"Username –∏ Key –Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω—ã.\")\n    print(\"–ó–∞–≥—Ä—É–∑–∏ —Ñ–∞–π–ª kaggle.json –∏–ª–∏ –∑–∞–ø–æ–ª–Ω–∏ KAGGLE_USERNAME –∏ KAGGLE_KEY –≤—ã—à–µ\")\n    try:\n        uploaded = files.upload()\n        !mkdir -p ~/.kaggle\n        !mv kaggle.json ~/.kaggle/\n        !chmod 600 ~/.kaggle/kaggle.json\n        print(\"‚úÖ Kaggle –Ω–∞—Å—Ç—Ä–æ–µ–Ω —á–µ—Ä–µ–∑ —Ñ–∞–π–ª\")\n    except:\n        print(\"‚ùå –§–∞–π–ª –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω. –ó–∞–ø–æ–ª–Ω–∏ KAGGLE_USERNAME –∏ KAGGLE_KEY\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–∫–∞—á–∏–≤–∞–µ–º dataset\n",
    "!kaggle datasets download -d jessicali9530/stanford-dogs-dataset\n",
    "!unzip -q stanford-dogs-dataset.zip -d /content/stanford_dogs\n",
    "!rm stanford-dogs-dataset.zip\n",
    "\n",
    "print(\"‚úÖ Stanford Dogs Dataset —Å–∫–∞—á–∞–Ω\")\n",
    "!ls /content/stanford_dogs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–∞—Ä–∏–∞–Ω—Ç B: –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ (–µ—Å–ª–∏ –Ω–µ—Ç Kaggle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—à—å –≤–∞—Ä–∏–∞–Ω—Ç B\n",
    "# !wget http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar\n",
    "# !mkdir -p /content/stanford_dogs/images\n",
    "# !tar -xf images.tar -C /content/stanford_dogs/\n",
    "# !rm images.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# –ü—É—Ç–∏\n",
    "RAW_DIR = Path(\"/content/stanford_dogs\")\n",
    "OUTPUT_DIR = Path(\"/content/breed_training\")\n",
    "\n",
    "# –ù–∞–π–¥—ë–º –ø–∞–ø–∫—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏\n",
    "if (RAW_DIR / \"images\" / \"Images\").exists():\n",
    "    IMAGES_DIR = RAW_DIR / \"images\" / \"Images\"\n",
    "elif (RAW_DIR / \"Images\").exists():\n",
    "    IMAGES_DIR = RAW_DIR / \"Images\"\n",
    "else:\n",
    "    # –ü–æ–∏—Å–∫\n",
    "    for p in RAW_DIR.rglob(\"n02*\"):\n",
    "        IMAGES_DIR = p.parent\n",
    "        break\n",
    "\n",
    "print(f\"–ü–∞–ø–∫–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏: {IMAGES_DIR}\")\n",
    "print(f\"–ù–∞–π–¥–µ–Ω–æ –ø–∞–ø–æ–∫ –ø–æ—Ä–æ–¥: {len(list(IMAGES_DIR.iterdir()))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_breed_name(folder_name: str) -> str:\n",
    "    \"\"\"–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Ä–æ–¥—ã –∏–∑ –∏–º–µ–Ω–∏ –ø–∞–ø–∫–∏.\"\"\"\n",
    "    if \"-\" in folder_name:\n",
    "        breed = folder_name.split(\"-\", 1)[1]\n",
    "    elif \"_\" in folder_name:\n",
    "        parts = folder_name.split(\"_\")\n",
    "        if parts[0].startswith(\"n\"):\n",
    "            breed = \"_\".join(parts[1:])\n",
    "        else:\n",
    "            breed = folder_name\n",
    "    else:\n",
    "        breed = folder_name\n",
    "    return breed.replace(\"_\", \" \")\n",
    "\n",
    "# –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –ø–æ—Ä–æ–¥—ã\n",
    "breeds_data = {}\n",
    "for breed_folder in sorted(IMAGES_DIR.iterdir()):\n",
    "    if not breed_folder.is_dir():\n",
    "        continue\n",
    "    breed_name = parse_breed_name(breed_folder.name)\n",
    "    images = list(breed_folder.glob(\"*.jpg\")) + list(breed_folder.glob(\"*.JPEG\"))\n",
    "    if images:\n",
    "        breeds_data[breed_name] = images\n",
    "\n",
    "print(f\"\\n–ù–∞–π–¥–µ–Ω–æ {len(breeds_data)} –ø–æ—Ä–æ–¥\")\n",
    "total_images = sum(len(imgs) for imgs in breeds_data.values())\n",
    "print(f\"–í—Å–µ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {total_images}\")\n",
    "\n",
    "# –ü–æ–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤\n",
    "print(\"\\n–ü—Ä–∏–º–µ—Ä—ã –ø–æ—Ä–æ–¥:\")\n",
    "for i, (breed, imgs) in enumerate(list(breeds_data.items())[:5]):\n",
    "    print(f\"  {breed}: {len(imgs)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°–æ–∑–¥–∞—ë–º –º–∞–ø–ø–∏–Ω–≥ ID ‚Üí –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–æ—Ä–æ–¥—ã\nsorted_breeds = sorted(breeds_data.keys())\nbreed_to_id = {breed: i for i, breed in enumerate(sorted_breeds)}\nid_to_breed = {i: breed for i, breed in enumerate(sorted_breeds)}\n\nprint(f\"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {len(id_to_breed)}\")\n\n# –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥\nbreeds_json = {str(k): v for k, v in id_to_breed.items()}\nwith open(\"/content/breeds.json\", \"w\") as f:\n    json.dump(breeds_json, f, indent=2)\nprint(\"‚úÖ breeds.json —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ /content/\")\n\n# –ö–æ–ø–∏—Ä—É–µ–º –≤ Google Drive (–µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á—ë–Ω)\ntry:\n    if os.path.exists(\"/content/drive/MyDrive\"):\n        os.makedirs(PROJECT_DIR, exist_ok=True)\n        shutil.copy(\"/content/breeds.json\", f\"{PROJECT_DIR}/breeds.json\")\n        print(f\"‚úÖ –°–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–æ –≤ Google Drive: {PROJECT_DIR}\")\n    else:\n        print(\"‚ö†Ô∏è Google Drive –Ω–µ –ø–æ–¥–∫–ª—é—á—ë–Ω - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ\")\nexcept Exception as e:\n    print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å –≤ Drive: {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val/test (80/10/10)\n",
    "np.random.seed(42)\n",
    "\n",
    "splits = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "for breed, images in breeds_data.items():\n",
    "    indices = np.random.permutation(len(images))\n",
    "    \n",
    "    train_end = int(len(images) * 0.8)\n",
    "    val_end = train_end + int(len(images) * 0.1)\n",
    "    \n",
    "    splits[\"train\"][breed] = [images[i] for i in indices[:train_end]]\n",
    "    splits[\"val\"][breed] = [images[i] for i in indices[train_end:val_end]]\n",
    "    splits[\"test\"][breed] = [images[i] for i in indices[val_end:]]\n",
    "\n",
    "# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n",
    "for split_name, data in splits.items():\n",
    "    total = sum(len(imgs) for imgs in data.values())\n",
    "    print(f\"{split_name}: {total} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–ø–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç ImageFolder\n",
    "print(\"–ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...\")\n",
    "\n",
    "for split_name, breeds in splits.items():\n",
    "    split_dir = OUTPUT_DIR / split_name\n",
    "    \n",
    "    for breed_name, images in tqdm(breeds.items(), desc=split_name):\n",
    "        breed_id = breed_to_id[breed_name]\n",
    "        safe_name = breed_name.replace(\" \", \"_\").replace(\"/\", \"-\")\n",
    "        breed_folder = split_dir / f\"{breed_id:03d}_{safe_name}\"\n",
    "        breed_folder.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        for i, img_path in enumerate(images):\n",
    "            dst = breed_folder / f\"{i:04d}.jpg\"\n",
    "            shutil.copy2(img_path, dst)\n",
    "\n",
    "print(\"\\n‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã!\")\n",
    "print(f\"   {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "\n",
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è\n",
    "CONFIG = {\n",
    "    \"model_name\": \"efficientnet_b4\",\n",
    "    \"num_classes\": len(id_to_breed),\n",
    "    \"img_size\": 224,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 30,\n",
    "    \"lr\": 1e-4,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "print(\"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(CONFIG[\"img_size\"], scale=(0.8, 1.0)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(CONFIG[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "print(\"‚úÖ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –î–∞—Ç–∞—Å–µ—Ç—ã\n",
    "train_dataset = datasets.ImageFolder(OUTPUT_DIR / \"train\", transform=train_transform)\n",
    "val_dataset = datasets.ImageFolder(OUTPUT_DIR / \"val\", transform=val_transform)\n",
    "test_dataset = datasets.ImageFolder(OUTPUT_DIR / \"test\", transform=val_transform)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"Val: {len(val_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "print(f\"Test: {len(test_dataset)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π\")\n",
    "\n",
    "# DataLoaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=CONFIG[\"batch_size\"], \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ DataLoaders —Å–æ–∑–¥–∞–Ω—ã\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ú–æ–¥–µ–ª—å\n",
    "model = timm.create_model(\n",
    "    CONFIG[\"model_name\"],\n",
    "    pretrained=True,\n",
    "    num_classes=CONFIG[\"num_classes\"],\n",
    ")\n",
    "model = model.to(CONFIG[\"device\"])\n",
    "\n",
    "# –ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"‚úÖ –ú–æ–¥–µ–ª—å: {CONFIG['model_name']}\")\n",
    "print(f\"   –í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}\")\n",
    "print(f\"   –û–±—É—á–∞–µ–º—ã—Ö: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ scheduler\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"lr\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=CONFIG[\"epochs\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"‚úÖ –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä: AdamW\")\n",
    "print(\"‚úÖ Scheduler: CosineAnnealing\")\n",
    "print(\"‚úÖ Loss: CrossEntropy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. –§—É–Ω–∫—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100.*correct/total:.1f}%\"})\n",
    "    \n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"–í–∞–ª–∏–¥–∞—Ü–∏—è —Å Top-1 –∏ Top-5 accuracy.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct_top1 = 0\n",
    "    correct_top5 = 0\n",
    "    total = 0\n",
    "    \n",
    "    for images, labels in tqdm(loader, desc=\"Validation\", leave=False):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Top-1\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct_top1 += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Top-5\n",
    "        _, top5_pred = outputs.topk(5, dim=1)\n",
    "        for i in range(labels.size(0)):\n",
    "            if labels[i] in top5_pred[i]:\n",
    "                correct_top5 += 1\n",
    "        \n",
    "        total += labels.size(0)\n",
    "    \n",
    "    return total_loss / total, correct_top1 / total, correct_top5 / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. –û–±—É—á–µ–Ω–∏–µ üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"train_acc\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"val_top1\": [],\n",
    "    \"val_top5\": [],\n",
    "}\n",
    "\n",
    "best_top5 = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"üöÄ –ù–ê–ß–ê–õ–û –û–ë–£–ß–ï–ù–ò–Ø\")\n",
    "print(\"=\"*60)\n",
    "print(f\"–≠–ø–æ—Ö: {CONFIG['epochs']}\")\n",
    "print(f\"Device: {CONFIG['device']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "for epoch in range(CONFIG[\"epochs\"]):\n    print(f\"\\nEpoch {epoch + 1}/{CONFIG['epochs']}\")\n    print(\"-\" * 40)\n    \n    # Training\n    train_loss, train_acc = train_epoch(\n        model, train_loader, criterion, optimizer, CONFIG[\"device\"]\n    )\n    \n    # Validation\n    val_loss, val_top1, val_top5 = validate(\n        model, val_loader, criterion, CONFIG[\"device\"]\n    )\n    \n    # Scheduler step\n    scheduler.step()\n    \n    # Save history\n    history[\"train_loss\"].append(train_loss)\n    history[\"train_acc\"].append(train_acc)\n    history[\"val_loss\"].append(val_loss)\n    history[\"val_top1\"].append(val_top1)\n    history[\"val_top5\"].append(val_top5)\n    \n    # Print metrics\n    print(f\"  Train Loss: {train_loss:.4f}, Acc: {train_acc*100:.2f}%\")\n    print(f\"  Val Loss: {val_loss:.4f}, Top-1: {val_top1*100:.2f}%, Top-5: {val_top5*100:.2f}%\")\n    \n    # Save best model\n    if val_top5 > best_top5:\n        best_top5 = val_top5\n        best_epoch = epoch + 1\n        torch.save(model.state_dict(), \"/content/breed_best.pt\")\n        print(f\"  ‚≠ê –ù–æ–≤—ã–π –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç! Top-5: {best_top5*100:.2f}%\")\n        \n        # –ö–æ–ø–∏—Ä—É–µ–º –≤ Google Drive (–µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á—ë–Ω)\n        try:\n            if os.path.exists(\"/content/drive/MyDrive\"):\n                os.makedirs(PROJECT_DIR, exist_ok=True)\n                shutil.copy(\"/content/breed_best.pt\", f\"{PROJECT_DIR}/breed_best.pt\")\n        except:\n            pass\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û\")\nprint(\"=\"*60)\nprint(f\"–õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: Top-5 = {best_top5*100:.2f}% (epoch {best_epoch})\")\nprint(f\"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: /content/breed_best.pt\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\n# Loss\naxes[0].plot(history[\"train_loss\"], label=\"Train\")\naxes[0].plot(history[\"val_loss\"], label=\"Val\")\naxes[0].set_title(\"Loss\")\naxes[0].set_xlabel(\"Epoch\")\naxes[0].legend()\naxes[0].grid(True)\n\n# Accuracy\naxes[1].plot([x*100 for x in history[\"train_acc\"]], label=\"Train\")\naxes[1].plot([x*100 for x in history[\"val_top1\"]], label=\"Val Top-1\")\naxes[1].set_title(\"Top-1 Accuracy\")\naxes[1].set_xlabel(\"Epoch\")\naxes[1].set_ylabel(\"%\")\naxes[1].legend()\naxes[1].grid(True)\n\n# Top-5\naxes[2].plot([x*100 for x in history[\"val_top5\"]], label=\"Val Top-5\", color=\"green\")\naxes[2].axhline(y=80, color=\"red\", linestyle=\"--\", label=\"Target (80%)\")\naxes[2].set_title(\"Top-5 Accuracy\")\naxes[2].set_xlabel(\"Epoch\")\naxes[2].set_ylabel(\"%\")\naxes[2].legend()\naxes[2].grid(True)\n\nplt.tight_layout()\nplt.savefig(\"/content/training_history.png\", dpi=150)\nplt.show()\n\n# –ö–æ–ø–∏—Ä—É–µ–º –≤ Google Drive (–µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á—ë–Ω)\ntry:\n    if os.path.exists(\"/content/drive/MyDrive\"):\n        shutil.copy(\"/content/training_history.png\", f\"{PROJECT_DIR}/training_history.png\")\n        print(f\"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ Google Drive\")\nexcept:\n    pass\nprint(\"‚úÖ –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: /content/training_history.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –Ω–∞ Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –ª—É—á—à—É—é –º–æ–¥–µ–ª—å\n",
    "model.load_state_dict(torch.load(\"/content/breed_best.pt\"))\n",
    "print(\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å\")\n",
    "\n",
    "# –û—Ü–µ–Ω–∫–∞ –Ω–∞ test set\n",
    "test_loss, test_top1, test_top5 = validate(\n",
    "    model, test_loader, criterion, CONFIG[\"device\"]\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ù–ê TEST SET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Top-1 Accuracy: {test_top1*100:.2f}%\")\n",
    "print(f\"Top-5 Accuracy: {test_top5*100:.2f}%\")\n",
    "print()\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ target\n",
    "if test_top5 > 0.80:\n",
    "    print(\"‚úÖ PASS: Top-5 > 80%\")\n",
    "else:\n",
    "    print(f\"‚ùå FAIL: Top-5 = {test_top5*100:.2f}% (target > 80%)\")\n",
    "    print(\"   –ü–æ–ø—Ä–æ–±—É–π —É–≤–µ–ª–∏—á–∏—Ç—å epochs –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –º–æ—â–Ω—É—é –º–æ–¥–µ–ª—å\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import random\nfrom PIL import Image\n\n# –í—ã–±–∏—Ä–∞–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è\nnum_samples = 6\nindices = random.sample(range(len(test_dataset)), num_samples)\n\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\naxes = axes.flatten()\n\nmodel.eval()\nfor i, idx in enumerate(indices):\n    img_path, true_label = test_dataset.samples[idx]\n    img = Image.open(img_path).convert(\"RGB\")\n    \n    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n    tensor = val_transform(img).unsqueeze(0).to(CONFIG[\"device\"])\n    with torch.no_grad():\n        output = model(tensor)\n        probs = torch.softmax(output, dim=1)[0]\n        top5_probs, top5_idx = probs.topk(5)\n    \n    pred_label = top5_idx[0].item()\n    pred_prob = top5_probs[0].item()\n    \n    true_name = id_to_breed[true_label]\n    pred_name = id_to_breed[pred_label]\n    \n    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ\n    axes[i].imshow(img)\n    color = \"green\" if pred_label == true_label else \"red\"\n    axes[i].set_title(\n        f\"True: {true_name}\\nPred: {pred_name} ({pred_prob*100:.1f}%)\",\n        color=color,\n        fontsize=10,\n    )\n    axes[i].axis(\"off\")\n\nplt.tight_layout()\nplt.savefig(\"/content/sample_predictions.png\", dpi=150)\nplt.show()\n\n# –ö–æ–ø–∏—Ä—É–µ–º –≤ Google Drive (–µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á—ë–Ω)\ntry:\n    if os.path.exists(\"/content/drive/MyDrive\"):\n        shutil.copy(\"/content/sample_predictions.png\", f\"{PROJECT_DIR}/sample_predictions.png\")\nexcept:\n    pass\nprint(\"‚úÖ –ü—Ä–∏–º–µ—Ä—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: /content/sample_predictions.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\nresults = {\n    \"config\": CONFIG,\n    \"best_epoch\": best_epoch,\n    \"val_top5\": best_top5,\n    \"test_top1\": test_top1,\n    \"test_top5\": test_top5,\n    \"history\": history,\n}\n\nwith open(\"/content/training_results.json\", \"w\") as f:\n    json.dump(results, f, indent=2)\n\nprint(\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã!\")\n\n# –ö–æ–ø–∏—Ä—É–µ–º –≤ Google Drive (–µ—Å–ª–∏ –ø–æ–¥–∫–ª—é—á—ë–Ω)\ndrive_connected = os.path.exists(\"/content/drive/MyDrive\")\nif drive_connected:\n    try:\n        shutil.copy(\"/content/training_results.json\", f\"{PROJECT_DIR}/training_results.json\")\n        print(f\"\\nüìÅ –§–∞–π–ª—ã –≤ Google Drive ({PROJECT_DIR}):\")\n    except:\n        drive_connected = False\n\nif not drive_connected:\n    print(f\"\\nüìÅ –§–∞–π–ª—ã –≤ /content/:\")\n\nprint(\"   - breed_best.pt (–≤–µ—Å–∞ –º–æ–¥–µ–ª–∏)\")\nprint(\"   - breeds.json (–º–∞–ø–ø–∏–Ω–≥ –ø–æ—Ä–æ–¥)\")\nprint(\"   - training_results.json (—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã)\")\nprint(\"   - training_history.png (–≥—Ä–∞—Ñ–∏–∫–∏)\")\nprint(\"   - sample_predictions.png (–ø—Ä–∏–º–µ—Ä—ã)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–∫–∞—á–∞–π –≤–µ—Å–∞ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä\n",
    "from google.colab import files\n",
    "\n",
    "print(\"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏...\")\n",
    "files.download(\"/content/breed_best.pt\")\n",
    "files.download(\"/content/breeds.json\")\n",
    "\n",
    "print(\"\\n‚úÖ –§–∞–π–ª—ã —Å–∫–∞—á–∞–Ω—ã!\")\n",
    "print(\"\\n–°–∫–æ–ø–∏—Ä—É–π –∏—Ö –≤ –ø—Ä–æ–µ–∫—Ç:\")\n",
    "print(\"  mv ~/Downloads/breed_best.pt models/breed.pt\")\n",
    "print(\"  mv ~/Downloads/breeds.json packages/models/breeds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ‚úÖ –ì–æ—Ç–æ–≤–æ!\n",
    "\n",
    "### –ß—Ç–æ –¥–∞–ª—å—à–µ:\n",
    "\n",
    "1. –°–∫–∞—á–∞–π `breed_best.pt` –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –∫–æ–º–ø—å—é—Ç–µ—Ä\n",
    "2. –°–∫–æ–ø–∏—Ä—É–π –≤ –ø—Ä–æ–µ–∫—Ç:\n",
    "   ```bash\n",
    "   mv ~/Downloads/breed_best.pt models/breed.pt\n",
    "   ```\n",
    "3. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π:\n",
    "   ```bash\n",
    "   python scripts/demo/test_pipeline.py test/ShihTzu-original.jpeg\n",
    "   ```\n",
    "\n",
    "### –ï—Å–ª–∏ Top-5 < 80%:\n",
    "- –£–≤–µ–ª–∏—á—å epochs –¥–æ 50\n",
    "- –ü–æ–ø—Ä–æ–±—É–π `efficientnet_b5` –∏–ª–∏ `vit_base_patch16_224`\n",
    "- –î–æ–±–∞–≤—å –±–æ–ª—å—à–µ augmentation"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}